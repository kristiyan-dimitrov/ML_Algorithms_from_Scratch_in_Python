{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "\n",
    "class SlotMachine:\n",
    "    def __init__(self, mean, std_dev):\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_dev\n",
    "\n",
    "    def pull(self):\n",
    "        return np.random.normal(self.mean, self.std_dev)\n",
    "\n",
    "\n",
    "class SlotMachines(gym.Env):\n",
    "    \"\"\"\n",
    "    Slot machine reinforcement learning environment for OpenAI Gym\n",
    "\n",
    "    Arguments:\n",
    "        n_machines - (int) Number of slot machines to create\n",
    "        mean_range - (tuple) Range of values for mean initialization\n",
    "        std_range - (tuple) Range of values for std initialization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_machines=10, mean_range=(-10, 10), std_range=(5, 10)):\n",
    "        # Initialize N slot machines with random means and std_devs\n",
    "        means = np.random.uniform(mean_range[0], mean_range[1], n_machines)\n",
    "        std_devs = np.random.uniform(std_range[0], std_range[1], n_machines)\n",
    "        self.machines = [SlotMachine(m, s) for (m, s) in zip(means, std_devs)]\n",
    "\n",
    "        # Required by OpenAI Gym\n",
    "        self.action_space = spaces.Discrete(n_machines)\n",
    "        self.observation_space = spaces.Discrete(1)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"\n",
    "        Seed the environment's random number generator\n",
    "\n",
    "        Arguments:\n",
    "          seed - (int) The random number generator seed.\n",
    "        \"\"\"\n",
    "        _, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform an action within the slot machine environment\n",
    "\n",
    "        Arguments:\n",
    "          action - (int) An action to perform\n",
    "\n",
    "        Returns:\n",
    "          observation - (int) The new environment state. This is always 0 for\n",
    "            SlotMachines.\n",
    "          reward - (float) The reward gained by taking an action.\n",
    "          done - (bool) Whether the environment has been completed and requires\n",
    "            resetting. This is always True for SlotMachines.\n",
    "          info - (dict) A dictionary of additional return values used for\n",
    "            debugging purposes.\n",
    "        \"\"\"\n",
    "        assert self.action_space.contains(action)\n",
    "        return 0, self.machines[action].pull(), True, {}\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment. For SlotMachines, this always returns 0.\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        \"\"\"\n",
    "        Render the environment display. For SlotMachines, this is a no-op.\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiArmedBandit:\n",
    "    \"\"\"\n",
    "    MultiArmedBandit reinforcement learning agent.\n",
    "\n",
    "    Arguments:\n",
    "      epsilon - (float) The probability of randomly exploring the action space\n",
    "        rather than exploiting the best action.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=0.2):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, env, steps=1000):\n",
    "        \"\"\"\n",
    "        Trains the MultiArmedBandit on an OpenAI Gym environment.\n",
    "\n",
    "        See page 32 of Sutton and Barto's book Reinformcement Learning for\n",
    "        pseudocode (http://incompleteideas.net/book/RLbook2018.pdf).\n",
    "        Initialize your parameters as all zeros. For the step size (alpha), use\n",
    "        1 / N, where N is the number of times the current action has been\n",
    "        performed. Use an epsilon-greedy policy for action selection.\n",
    "\n",
    "        See (https://gym.openai.com/) for examples of how to use the OpenAI\n",
    "        Gym Environment interface.\n",
    "\n",
    "        Hints:\n",
    "          - Use env.action_space.n and env.observation_space.n to get the\n",
    "            number of available actions and states, respectively.\n",
    "          - Remember to reset your environment at the end of each episode. To\n",
    "            do this, call env.reset() whenever the value of \"done\" returned\n",
    "            from env.step() is True.\n",
    "          - If all values of a np.array are equal, np.argmax deterministically\n",
    "            returns 0.\n",
    "          - In order to avoid non-deterministic tests, use only np.random for\n",
    "            random number generation.\n",
    "          - MultiArmedBandit treats all environment states the same. However,\n",
    "            in order to have the same API as agents that model state, you must\n",
    "            explicitly return the state-action-values Q(s, a). To do so, just\n",
    "            copy the action values learned by MultiArmedBandit S times, where\n",
    "            S is the number of states.\n",
    "\n",
    "        Arguments:\n",
    "          env - (Env) An OpenAI Gym environment with discrete actions and\n",
    "            observations. See the OpenAI Gym documentation for example use\n",
    "            cases (https://gym.openai.com/docs/).\n",
    "          steps - (int) The number of actions to perform within the environment\n",
    "            during training.\n",
    "\n",
    "        Returns:\n",
    "          state_action_values - (np.array) The values assigned by the algorithm\n",
    "            to each state-action pair as a 2D numpy array. The dimensionality\n",
    "            of the numpy array should be S x A, where S is the number of\n",
    "            states in the environment and A is the number of possible actions.\n",
    "          rewards - (np.array) A 1D sequence of averaged rewards of length 100.\n",
    "            Let s = np.floor(steps / 100), then rewards[0] should contain the\n",
    "            average reward over the first s steps, rewards[1] should contain\n",
    "            the average reward over the next s steps, etc.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_space = SlotMachines()\n",
    "sample_space.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a type of environment with 10 different discreet actions?\n",
    "sample_space.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the observation_space i.e. how many possible locations there are in the Markov chain?\n",
    "sample_space.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, -11.31862659734954, True, {})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does the same as the one below, but returns a few additional parameters, which aren't really relevant here\n",
    "sample_space.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.4266823552975225"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a reward from the given space i.e. from a specific machine\n",
    "sample_space.machines[0].pull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random possible action\n",
    "sample_space.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10.91543829240107"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This combines the two above\n",
    "# First we take a random action value, \n",
    "# Then we sample from the corresponding slot machine\n",
    "sample_space.machines[sample_space.action_space.sample()].pull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From __init__.py in src\n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='{}-{}'.format('SlotMachines', 'v0'),\n",
    "    entry_point='src:{}'.format('SlotMachines'),\n",
    "    max_episode_steps=1,\n",
    "    nondeterministic=True)\n",
    "\n",
    "# register(\n",
    "#     id='FrozonLakeNoSlippery-v0',\n",
    "#     entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "#     kwargs={'map_name': '4x4', 'is_slippery': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('SlotMachines-v0', n_machines=10, mean_range=(-10, 10), std_range=(5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_rewards = np.zeros((1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(state_action_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4224418374352119"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([1,2,3,4,5,6,7], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions=10\n",
    "N = np.zeros((1, n_actions)) # how many times has each action been selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[:, 3] +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.random.randint(0,10,1000)\n",
    "s = np.floor(1000 / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rewards = np.array([np.mean(rewards[int(interval*s):int((interval+1)*s)]) for interval in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 5, 3, 4, 4, 8, 6, 8, 6])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randint(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_values = np.zeros((1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.argmax(state_action_values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.flatten()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_values = np.array([\n",
    "        [0.0, 0.7, 0.3, 0.0],\n",
    "        [0.0, 1.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 1.0, 0.0],\n",
    "        [0.0, 1.0, 0.0, 0.0],\n",
    "        [0.0, 0.51, 0.49, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0],\n",
    "        [0.5, 0.0, 0.5, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 0.2, 0.8, 0.0],\n",
    "        [0.0, 0.2, 0.8, 0.0],\n",
    "        [0.0, 0.6, 0.4, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0],\n",
    "        [1.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 1.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 0.0]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(state_action_values[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.7, 0.3, 0. ])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_values[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not step % 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw6-reinforcement-learning",
   "language": "python",
   "name": "hw6-reinforcement-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
