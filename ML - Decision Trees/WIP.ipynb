{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import csv\n",
    "import timeit\n",
    "\n",
    "def load_data(data_path):\n",
    "    \"\"\"\n",
    "    Associated test case: tests/test_data.py\n",
    "\n",
    "    Reading and manipulating data is a vital skill for machine learning.\n",
    "\n",
    "    This function loads the data in data_path csv into two numpy arrays:\n",
    "    features (of size NxK) and targets (of size Nx1) where N is the number of rows\n",
    "    and K is the number of features. \n",
    "    \n",
    "    data_path leads to a csv comma-delimited file with each row corresponding to a \n",
    "    different example. Each row contains binary features for each example \n",
    "    (e.g. chocolate, fruity, caramel, etc.) The last column indicates the label for the\n",
    "    example how likely it is to win a head-to-head matchup with another candy \n",
    "    bar.\n",
    "\n",
    "    This function reads in the csv file, and reads each row into two numpy arrays.\n",
    "    The first array contains the features for each row. For example, in candy-data.csv\n",
    "    the features are:\n",
    "\n",
    "    chocolate,fruity,caramel,peanutyalmondy,nougat,crispedricewafer,hard,bar,pluribus\n",
    "\n",
    "    The second array contains the targets for each row. The targets are in the last \n",
    "    column of the csv file (labeled 'class'). The first row of the csv file contains \n",
    "    the labels for each column and shouldn't be read into an array.\n",
    "\n",
    "    Example:\n",
    "    chocolate,fruity,caramel,peanutyalmondy,nougat,crispedricewafer,hard,bar,pluribus,class\n",
    "    1,0,1,0,0,1,0,1,0,1\n",
    "\n",
    "    should be turned into:\n",
    "\n",
    "    [1,0,1,0,0,1,0,1,0] (features) and [1] (targets).\n",
    "\n",
    "    This should be done for each row in the csv file, making arrays of size NxK and Nx1.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): path to csv file containing the data\n",
    "\n",
    "    Output:\n",
    "        features (np.array): numpy array of size NxK containing the K features\n",
    "        targets (np.array): numpy array of size 1xN containing the N targets.\n",
    "        attribute_names (list): list of strings containing names of each attribute \n",
    "            (headers of csv)\n",
    "    \"\"\"\n",
    "\n",
    "    with open(data_path, 'r', newline = '') as csvfile:\n",
    "        data = csv.reader(csvfile, delimiter = ',')\n",
    "        attribute_names = data.__next__()\n",
    "\n",
    "        col_len = len(attribute_names) - 1 # Number of features\n",
    "    \n",
    "        attribute_names = attribute_names[:-1]\n",
    "        features = list()\n",
    "        targets = list()\n",
    "    \n",
    "        for row in data:\n",
    "            features.append([float(el) for el in row[:col_len]]) \n",
    "            targets.append(int(row[col_len]))\n",
    "        \n",
    "    return np.array(features), np.array(targets), attribute_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/candy-data.csv'\n",
    "data = load_data(data_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[76, 46, 50, 38, 35]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# data = np.array(list(range(5,25)))\n",
    "\n",
    "\n",
    "test_ind = random.sample(range(data.shape[0]), 5)\n",
    "test_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 1., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = list(set(range(data.shape[0])) - set(test_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1., 0., 0., 1., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 1., 1., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 1., 0., 1., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 0., 1., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [1., 0., 1., 1., 1., 0., 0., 1., 0.],\n",
       "       [1., 0., 1., 1., 0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 1., 0., 0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 1., 0., 0., 1.]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[train_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = np.array([1,1,1,0])\n",
    "pred = np.array([0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_m = confusion_matrix(actual, pred)\n",
    "precision = conf_m[1,1] / sum(conf_m[:,1])\n",
    "recall = conf_m[1,1] / sum(conf_m[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_proba(targets:np.array, class_:float):\n",
    "\n",
    "    return sum(targets==class_)/len(targets)\n",
    "\n",
    "def entropy(features, targets):\n",
    "\n",
    "    # entropy = 0 for entirely homogeneous nodes i.e. nodes with just 1 class\n",
    "    if len(list(set(targets))) == 1:\n",
    "        return 0\n",
    "\n",
    "    return sum([ -prior_proba(targets, class_) * np.log2(prior_proba(targets, class_))  for class_ in list(set(targets))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([1,1,1,1,1,1,1,1,1,1])\n",
    "sum(arr == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "entropy([1], arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_proba(targets:np.array, class_:float):\n",
    "\n",
    "    return sum(targets==class_)/len(targets)\n",
    "\n",
    "def entropy(targets):\n",
    "\n",
    "    # entropy = 0 for entirely homogeneous nodes i.e. nodes with just 1 class\n",
    "    if len(list(set(targets))) == 1:\n",
    "        return 0\n",
    "\n",
    "    return sum([ -prior_proba(targets, class_) * np.log2(prior_proba(targets, class_))  for class_ in list(set(targets))])\n",
    "\n",
    "def information_gain(features, attribute_index, targets):\n",
    "    \"\"\"\n",
    "    TODO: Implement me!\n",
    "\n",
    "    Information gain is how a decision tree makes decisions on how to create\n",
    "    split points in the tree. Information gain is measured in terms of entropy.\n",
    "    The goal of a decision tree is to decrease entropy at each split point as much as\n",
    "    possible. This function should work perfectly or your decision tree will not work\n",
    "    properly.\n",
    "\n",
    "    Information gain is a central concept in many machine learning algorithms. In\n",
    "    decision trees, it captures how effective splitting the tree on a specific attribute\n",
    "    will be for the goal of classifying the training data correctly. Consider\n",
    "    data points S and an attribute A. S is split into two data points given binary A:\n",
    "\n",
    "        S(A == 0) and S(A == 1)\n",
    "\n",
    "    Together, the two subsets make up S. If A was an attribute perfectly correlated with\n",
    "    the class of each data point in S, then all points in a given subset will have the\n",
    "    same class. Clearly, in this case, we want something that captures that A is a good\n",
    "    attribute to use in the decision tree. This something is information gain. Formally:\n",
    "\n",
    "        IG(S,A) = H(S) - H(S|A)\n",
    "\n",
    "    where H is information entropy. Recall that entropy captures how orderly or chaotic\n",
    "    a system is. A system that is very chaotic will evenly distribute probabilities to\n",
    "    all outcomes (e.g. 50% chance of class 0, 50% chance of class 1). Machine learning\n",
    "    algorithms work to decrease entropy, as that is the only way to make predictions\n",
    "    that are accurate on testing data. Formally, H is defined as:\n",
    "\n",
    "        H(S) = sum_{c in (classes in S)} -p(c) * log_2 p(c)\n",
    "\n",
    "    To elaborate: for each class in S, you compute its prior probability p(c):\n",
    "\n",
    "        (# of elements of class c in S) / (total # of elements in S)\n",
    "\n",
    "    Then you compute the term for this class:\n",
    "\n",
    "        -p(c) * log_2 p(c)\n",
    "\n",
    "    Then compute the sum across all classes. The final number is the entropy. To gain\n",
    "    more intution about entropy, consider the following - what does H(S) = 0 tell you\n",
    "    about S?\n",
    "\n",
    "    Information gain is an extension of entropy. The equation for information gain\n",
    "    involves comparing the entropy of the set and the entropy of the set when conditioned\n",
    "    on selecting for a single attribute (e.g. S(A == 0)).\n",
    "\n",
    "    For more details: https://en.wikipedia.org/wiki/ID3_algorithm#The_ID3_metrics\n",
    "\n",
    "    Args:\n",
    "        features (np.array): numpy array containing features for each example.\n",
    "        attribute_index (int): which column of features to take when computing the\n",
    "            information gain\n",
    "        targets (np.array): numpy array containing labels corresponding to each example.\n",
    "\n",
    "    Output:\n",
    "        information_gain (float): information gain if the features were split on the\n",
    "            attribute_index.\n",
    "    \"\"\"\n",
    "\n",
    "    parent_entropy = entropy(targets=targets)\n",
    "\n",
    "    feature_classes = list(set(features[:,attribute_index]))\n",
    "\n",
    "    # if the selected feature has only 1 class in it, then it will not separate anything\n",
    "    # i.e. it will not give any information\n",
    "    if len(feature_classes) == 1:\n",
    "        return 0\n",
    "\n",
    "\n",
    "    child_entropies = [(sum(features[:,attribute_index]==feature_class) / len(targets)) * entropy(targets[features[:,attribute_index]==feature_class]) for feature_class in feature_classes]\n",
    "\n",
    "    split_entropy = sum(child_entropies)\n",
    "\n",
    "    return parent_entropy - split_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31127812445913283"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = np.array([1,0,1,0])\n",
    "features = np.array([[1,1],[1,0],[0,1], [1,0]])\n",
    "\n",
    "information_gain(features, 0, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[features[:,0] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31127812445913283"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - ((-np.log2(2/3) * 2/3 - 1/3 * np.log2(1/3)) * 3/4 + 1/4 * 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(features[:,0] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pred[actual==0]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = np.array([1,2,3,4,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act[[True, False, False, True, False]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_m = np.array([[1,3],[2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_m[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_proba(targets:np.array, class_:float):\n",
    "\n",
    "    return sum(targets==class_)/len(targets)\n",
    "\n",
    "def entropy(targets):\n",
    "\n",
    "    # entropy = 0 for entirely homogeneous nodes i.e. nodes with just 1 class\n",
    "    if len(list(set(targets))) == 1:\n",
    "        return 0\n",
    "\n",
    "    return sum([ -prior_proba(targets, class_) * np.log2(prior_proba(targets, class_))  for class_ in list(set(targets))])\n",
    "\n",
    "\n",
    "def information_gain(features, attribute_index, targets):\n",
    "    \"\"\"\n",
    "    TODO: Implement me!\n",
    "\n",
    "    Information gain is how a decision tree makes decisions on how to create\n",
    "    split points in the tree. Information gain is measured in terms of entropy.\n",
    "    The goal of a decision tree is to decrease entropy at each split point as much as\n",
    "    possible. This function should work perfectly or your decision tree will not work\n",
    "    properly.\n",
    "\n",
    "    Information gain is a central concept in many machine learning algorithms. In\n",
    "    decision trees, it captures how effective splitting the tree on a specific attribute\n",
    "    will be for the goal of classifying the training data correctly. Consider\n",
    "    data points S and an attribute A. S is split into two data points given binary A:\n",
    "\n",
    "        S(A == 0) and S(A == 1)\n",
    "\n",
    "    Together, the two subsets make up S. If A was an attribute perfectly correlated with\n",
    "    the class of each data point in S, then all points in a given subset will have the\n",
    "    same class. Clearly, in this case, we want something that captures that A is a good\n",
    "    attribute to use in the decision tree. This something is information gain. Formally:\n",
    "\n",
    "        IG(S,A) = H(S) - H(S|A)\n",
    "\n",
    "    where H is information entropy. Recall that entropy captures how orderly or chaotic\n",
    "    a system is. A system that is very chaotic will evenly distribute probabilities to\n",
    "    all outcomes (e.g. 50% chance of class 0, 50% chance of class 1). Machine learning\n",
    "    algorithms work to decrease entropy, as that is the only way to make predictions\n",
    "    that are accurate on testing data. Formally, H is defined as:\n",
    "\n",
    "        H(S) = sum_{c in (classes in S)} -p(c) * log_2 p(c)\n",
    "\n",
    "    To elaborate: for each class in S, you compute its prior probability p(c):\n",
    "\n",
    "        (# of elements of class c in S) / (total # of elements in S)\n",
    "\n",
    "    Then you compute the term for this class:\n",
    "\n",
    "        -p(c) * log_2 p(c)\n",
    "\n",
    "    Then compute the sum across all classes. The final number is the entropy. To gain\n",
    "    more intution about entropy, consider the following - what does H(S) = 0 tell you\n",
    "    about S?\n",
    "\n",
    "    Information gain is an extension of entropy. The equation for information gain\n",
    "    involves comparing the entropy of the set and the entropy of the set when conditioned\n",
    "    on selecting for a single attribute (e.g. S(A == 0)).\n",
    "\n",
    "    For more details: https://en.wikipedia.org/wiki/ID3_algorithm#The_ID3_metrics\n",
    "\n",
    "    Args:\n",
    "        features (np.array): numpy array containing features for each example.\n",
    "        attribute_index (int): which column of features to take when computing the\n",
    "            information gain\n",
    "        targets (np.array): numpy array containing labels corresponding to each example.\n",
    "\n",
    "    Output:\n",
    "        information_gain (float): information gain if the features were split on the\n",
    "            attribute_index.\n",
    "    \"\"\"\n",
    "\n",
    "    parent_entropy = entropy(targets)\n",
    "\n",
    "    feature_classes = list(set(features[:,attribute_index]))\n",
    "\n",
    "    # if the selected feature has only 1 class in it, then it will not separate anything\n",
    "    # i.e. it will not give any information\n",
    "    if len(feature_classes) == 1:\n",
    "        return 0\n",
    "\n",
    "    child_entropies = [(sum(features[:,attribute_index]==feature_class) / len(targets)) * entropy(targets[features[:,attribute_index]==feature_class]) for feature_class in feature_classes]\n",
    "\n",
    "    split_entropy = sum(child_entropies)\n",
    "\n",
    "    return parent_entropy - split_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, value=None, attribute_name=\"root\", attribute_index=None, branches=None):\n",
    "        \"\"\"\n",
    "        This class implements a tree structure with multiple branches at each node.\n",
    "        If self.branches is an empty list, this is a leaf node and what is contained in\n",
    "        self.value is the predicted class.\n",
    "\n",
    "        The defaults for this are for a root node in the tree.\n",
    "\n",
    "        Arguments:\n",
    "            branches (list): List of Tree classes. Used to traverse the tree. In a\n",
    "                binary decision tree, the length of this list is either 2 (for left and\n",
    "                right branches) or 0 (at a leaf node).\n",
    "            attribute_name (str): Contains name of attribute that the tree splits the data\n",
    "                on. Used for visualization (see `DecisionTree.visualize`).\n",
    "            attribute_index (float): Contains the  index of the feature vector for the\n",
    "                given attribute. Should match with self.attribute_name.\n",
    "            value (number): Contains the value that data should be compared to along the\n",
    "                given attribute.\n",
    "        \"\"\"\n",
    "        self.branches = [] if branches is None else branches\n",
    "        self.attribute_name = attribute_name\n",
    "        self.attribute_index = attribute_index\n",
    "        self.value = value\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, attribute_names):\n",
    "        \"\"\"\n",
    "        TODO: Implement this class.\n",
    "\n",
    "        This class implements a binary decision tree learner for examples with\n",
    "        categorical attributes. Use the ID3 algorithm for implementing the Decision\n",
    "        Tree: https://en.wikipedia.org/wiki/ID3_algorithm\n",
    "\n",
    "        A decision tree is a machine learning model that fits data with a tree\n",
    "        structure. Each branching point along the tree marks a decision (e.g.\n",
    "        today is sunny or today is not sunny). Data is filtered by the value of\n",
    "        each attribute to the next level of the tree. At the next level, the process\n",
    "        starts again with the remaining attributes, recursing on the filtered data.\n",
    "\n",
    "        Which attributes to split on at each point in the tree are decided by the\n",
    "        information gain of a specific attribute.\n",
    "\n",
    "        Here, you will implement a binary decision tree that uses the ID3 algorithm.\n",
    "        Your decision tree will be contained in `self.tree`, which consists of\n",
    "        nested Tree classes (see above).\n",
    "\n",
    "        Args:\n",
    "            attribute_names (list): list of strings containing the attribute names for\n",
    "                each feature (e.g. chocolatey, good_grades, etc.)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attribute_names = attribute_names\n",
    "        self.tree = None\n",
    "\n",
    "    def _check_input(self, features):\n",
    "        if features.shape[1] != len(self.attribute_names):\n",
    "            raise ValueError(\n",
    "                \"Number of features and number of attribute names must match!\"\n",
    "            )\n",
    "\n",
    "    def fit(self, features, targets):\n",
    "        \"\"\"\n",
    "        Takes in the features as a numpy array and fits a decision tree to the targets.\n",
    "\n",
    "        Args:\n",
    "            features (np.array): numpy array of size NxF containing features, where N is\n",
    "                number of examples and F is number of features.\n",
    "            targets (np.array): numpy array containing class labels for each of the N\n",
    "                examples.\n",
    "        Output:\n",
    "            VOID: It should update self.node with a built decision tree.\n",
    "        \"\"\"\n",
    "        # Need to handle this special case where I have just one feature left\n",
    "        # And it's getting cast as a 1-dim vector\n",
    "        # I need it to stay as a 2-dim vector with 1 column\n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(-1,1)\n",
    "        \n",
    "        inf_gains = [information_gain(features, index, targets) for index in range(features.shape[1])]\n",
    "\n",
    "        max_gain = max(inf_gains) if len(inf_gains) != 0 else 0\n",
    "        \n",
    "        # Check that the number of columns in features and number of attribute names matches\n",
    "        self._check_input(features)\n",
    "        \n",
    "        if (len(self.attribute_names) == 0) or (len(list(set(targets))) == 1): \n",
    "#             print(\"NO ATTRIBUTES LEFT OR TARGETS ARE ALL SAME\")\n",
    "            \n",
    "            zeros = sum(targets == 0)\n",
    "            ones = sum(targets == 1)\n",
    "            pred_value = 1\n",
    "            if zeros >= ones:\n",
    "                pred_value = 0\n",
    "            self.tree = Node(value = pred_value, attribute_name='leaf')\n",
    "        \n",
    "        elif max_gain != 0:\n",
    "            # Finding the first feature with max gain \n",
    "            # There might be more, but we just take the first one since it doesn't really matter\n",
    "            for index in range(len(inf_gains)):\n",
    "                if inf_gains[index] == max_gain:\n",
    "                # the value of index is now the index where the max_gain is\n",
    "                    break\n",
    "            print(f\"INDEX: {index}\")\n",
    "            self.tree = Node(value=0, attribute_name=self.attribute_names[index], attribute_index=index)\n",
    "            \n",
    "            remaining_attribute_names = [self.attribute_names[i] for i in range(len(self.attribute_names)) if i != index ]\n",
    "            remaining_attribute_indices = [ i for i in range(len(self.attribute_names)) if i != index]\n",
    "        \n",
    "            left_features = features[features[:,index] <= self.tree.value, :][:,remaining_attribute_indices]\n",
    "            left_targets = targets[features[:,index] <= self.tree.value]\n",
    "        \n",
    "            right_features = features[features[:,index] > self.tree.value, :][:,remaining_attribute_indices]\n",
    "            right_targets = targets[features[:,index] > self.tree.value]\n",
    "        \n",
    "            print(\"---- GOING DEEPER ON RECURSION ----\")\n",
    "            print(\"FEATURES\")\n",
    "            print(left_features)\n",
    "            print(right_features)\n",
    "            print(\"TARGETS\")\n",
    "            print(left_targets, right_targets)\n",
    "            \n",
    "            left_tree = DecisionTree(remaining_attribute_names)\n",
    "            left_tree.fit(left_features, left_targets)\n",
    "            self.tree.branches.append(left_tree)\n",
    "        \n",
    "            right_tree = DecisionTree(remaining_attribute_names)\n",
    "            right_tree.fit(right_features, right_targets)\n",
    "            self.tree.branches.append(right_tree)\n",
    "        \n",
    "        else:\n",
    "            zeros = sum(targets == 0)\n",
    "            ones = sum(targets == 1)\n",
    "            pred_value = 1\n",
    "            if zeros >= ones:\n",
    "                pred_value = 0\n",
    "            self.tree = Node(value = pred_value, attribute_name='leaf')\n",
    "            \n",
    "            \n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Takes in features as a numpy array and predicts classes for each point using\n",
    "        the trained model.\n",
    "\n",
    "        Args:\n",
    "            features (np.array): numpy array of size NxF containing features, where N is\n",
    "                number of examples and F is number of features.\n",
    "        Outputs:\n",
    "            predictions (np.array): numpy array of size N array which has the predicitons \n",
    "            for the input data.\n",
    "        \"\"\"\n",
    "        print(f'INPUT FEATURES: {features}')\n",
    "        \n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(1,-1)\n",
    "        \n",
    "        if features.shape[0] != 1:\n",
    "            self._check_input(features)\n",
    "        \n",
    "        print(f'RESHAPED FEATURES: {features}')\n",
    "        \n",
    "        predictions = np.full(features.shape[0], np.nan, dtype=int)\n",
    "        \n",
    "        print(f'PREDICTIONS BEFORE: {predictions}')\n",
    "        \n",
    "        for i in range(features.shape[0]):\n",
    "            if self.tree.attribute_index is None:\n",
    "                return self.tree.value\n",
    "            else:\n",
    "                print(f\"ATTRIBUTE_INDEX: {self.tree.attribute_index}\")\n",
    "                if features[i][self.tree.attribute_index] == 0:\n",
    "                    predictions[i] = self.tree.branches[0].predict(features[i])\n",
    "                else:\n",
    "                    predictions[i] = self.tree.branches[1].predict(features[i])\n",
    "                    \n",
    "        return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE ABOVE KIND-OFF WORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX: 2\n",
      "---- GOING DEEPER ON RECURSION ----\n",
      "FEATURES\n",
      "[[0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 1]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "TARGETS\n",
      "[1 0 0 0 0] [1 1 1]\n",
      "INDEX: 0\n",
      "---- GOING DEEPER ON RECURSION ----\n",
      "FEATURES\n",
      "[[1]\n",
      " [0]\n",
      " [1]]\n",
      "[[1]\n",
      " [1]]\n",
      "TARGETS\n",
      "[1 0 0] [0 0]\n",
      "INDEX: 0\n",
      "---- GOING DEEPER ON RECURSION ----\n",
      "FEATURES\n",
      "[]\n",
      "[]\n",
      "TARGETS\n",
      "[0] [1 0]\n"
     ]
    }
   ],
   "source": [
    "example_attributes = ['feature_1', 'feature_2', 'feature_3']\n",
    "features = np.array([[1,0,1], [0,1,0],[1,0,1],[1,1,1],[1,1,0],[1,1,0],[0,0,0],[0,1,0]])\n",
    "targets = np.array([1,1,1,1,0,0,0,0])\n",
    "\n",
    "example_DT = DecisionTree(example_attributes)\n",
    "\n",
    "example_DT.fit(features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1],\n",
       "       [0, 0, 0],\n",
       "       [0, 1, 1],\n",
       "       [1, 1, 0],\n",
       "       [0, 1, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = np.array([[1,0,1],[0,0,0],[0,1,1],[1,1,0],[0,1,1],[1,1,1]])\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT FEATURES: [[1 0 1]\n",
      " [0 0 0]\n",
      " [0 1 1]\n",
      " [1 1 0]\n",
      " [0 1 1]\n",
      " [1 1 1]]\n",
      "RESHAPED FEATURES: [[1 0 1]\n",
      " [0 0 0]\n",
      " [0 1 1]\n",
      " [1 1 0]\n",
      " [0 1 1]\n",
      " [1 1 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808 -9223372036854775808 -9223372036854775808\n",
      " -9223372036854775808 -9223372036854775808 -9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-243-e7736b05780a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexample_DT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-240-ac411f55f023>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "example_DT.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1]\n",
      " [0 1 1]\n",
      " [0 0 1]\n",
      " [0 1 1]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [0 0 0]\n",
      " [0 1 0]]\n",
      "INDEX: 0\n",
      "---- GOING DEEPER ON RECURSION ----\n",
      "FEATURES\n",
      "[[1 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]]\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "TARGETS\n",
      "[0 1 0 0 0] [1 0 1]\n",
      "INDEX: 0\n",
      "---- GOING DEEPER ON RECURSION ----\n",
      "FEATURES\n",
      "[[1]\n",
      " [0]]\n",
      "[[1]\n",
      " [1]\n",
      " [0]]\n",
      "TARGETS\n",
      "[1 0] [0 0 0]\n",
      "INDEX: 0\n",
      "---- GOING DEEPER ON RECURSION ----\n",
      "FEATURES\n",
      "[]\n",
      "[]\n",
      "TARGETS\n",
      "[0] [1]\n",
      "INDEX: 0\n",
      "---- GOING DEEPER ON RECURSION ----\n",
      "FEATURES\n",
      "[[1]]\n",
      "[[0]\n",
      " [0]]\n",
      "TARGETS\n",
      "[1] [0 1]\n"
     ]
    }
   ],
   "source": [
    "example_attributes = ['feature_1', 'feature_2', 'feature_3']\n",
    "features = np.array([[1,0,1], [0,1,1],[0,0,1],[0,1,1],[1,1,0],[1,1,0],[0,0,0],[0,1,0]])\n",
    "targets = np.array([1,0,1,0,0,1,0,0])\n",
    "print(features)\n",
    "example_DT = DecisionTree(example_attributes)\n",
    "\n",
    "example_DT.fit(features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT FEATURES: [[1 0 1]\n",
      " [0 0 0]\n",
      " [0 1 1]\n",
      " [1 1 0]\n",
      " [0 1 1]\n",
      " [1 1 1]]\n",
      "RESHAPED FEATURES: [[1 0 1]\n",
      " [0 0 0]\n",
      " [0 1 1]\n",
      " [1 1 0]\n",
      " [0 1 1]\n",
      " [1 1 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808 -9223372036854775808 -9223372036854775808\n",
      " -9223372036854775808 -9223372036854775808 -9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "INPUT FEATURES: [1 0 1]\n",
      "RESHAPED FEATURES: [[1 0 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "INPUT FEATURES: [1 0 1]\n",
      "RESHAPED FEATURES: [[1 0 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "INPUT FEATURES: [0 0 0]\n",
      "RESHAPED FEATURES: [[0 0 0]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "INPUT FEATURES: [0 0 0]\n",
      "RESHAPED FEATURES: [[0 0 0]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "INPUT FEATURES: [0 0 0]\n",
      "RESHAPED FEATURES: [[0 0 0]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "INPUT FEATURES: [0 1 1]\n",
      "RESHAPED FEATURES: [[0 1 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "INPUT FEATURES: [0 1 1]\n",
      "RESHAPED FEATURES: [[0 1 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "INPUT FEATURES: [0 1 1]\n",
      "RESHAPED FEATURES: [[0 1 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "INPUT FEATURES: [1 1 0]\n",
      "RESHAPED FEATURES: [[1 1 0]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "INPUT FEATURES: [1 1 0]\n",
      "RESHAPED FEATURES: [[1 1 0]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "INPUT FEATURES: [0 1 1]\n",
      "RESHAPED FEATURES: [[0 1 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "INPUT FEATURES: [0 1 1]\n",
      "RESHAPED FEATURES: [[0 1 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "INPUT FEATURES: [0 1 1]\n",
      "RESHAPED FEATURES: [[0 1 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "INPUT FEATURES: [1 1 1]\n",
      "RESHAPED FEATURES: [[1 1 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "INPUT FEATURES: [1 1 1]\n",
      "RESHAPED FEATURES: [[1 1 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_DT.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRYING TO SALVAGE ABOVE\n",
    "## Appears to work, this is now CURRENT VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, value=None, attribute_name=\"root\", attribute_index=None, branches=None):\n",
    "        \"\"\"\n",
    "        This class implements a tree structure with multiple branches at each node.\n",
    "        If self.branches is an empty list, this is a leaf node and what is contained in\n",
    "        self.value is the predicted class.\n",
    "\n",
    "        The defaults for this are for a root node in the tree.\n",
    "\n",
    "        Arguments:\n",
    "            branches (list): List of Tree classes. Used to traverse the tree. In a\n",
    "                binary decision tree, the length of this list is either 2 (for left and\n",
    "                right branches) or 0 (at a leaf node).\n",
    "            attribute_name (str): Contains name of attribute that the tree splits the data\n",
    "                on. Used for visualization (see `DecisionTree.visualize`).\n",
    "            attribute_index (float): Contains the  index of the feature vector for the\n",
    "                given attribute. Should match with self.attribute_name.\n",
    "            value (number): Contains the value that data should be compared to along the\n",
    "                given attribute.\n",
    "        \"\"\"\n",
    "        self.branches = [] if branches is None else branches\n",
    "        self.attribute_name = attribute_name\n",
    "        self.attribute_index = attribute_index\n",
    "        self.value = value\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, attribute_names):\n",
    "        \"\"\"\n",
    "        TODO: Implement this class.\n",
    "\n",
    "        This class implements a binary decision tree learner for examples with\n",
    "        categorical attributes. Use the ID3 algorithm for implementing the Decision\n",
    "        Tree: https://en.wikipedia.org/wiki/ID3_algorithm\n",
    "\n",
    "        A decision tree is a machine learning model that fits data with a tree\n",
    "        structure. Each branching point along the tree marks a decision (e.g.\n",
    "        today is sunny or today is not sunny). Data is filtered by the value of\n",
    "        each attribute to the next level of the tree. At the next level, the process\n",
    "        starts again with the remaining attributes, recursing on the filtered data.\n",
    "\n",
    "        Which attributes to split on at each point in the tree are decided by the\n",
    "        information gain of a specific attribute.\n",
    "\n",
    "        Here, you will implement a binary decision tree that uses the ID3 algorithm.\n",
    "        Your decision tree will be contained in `self.tree`, which consists of\n",
    "        nested Tree classes (see above).\n",
    "\n",
    "        Args:\n",
    "            attribute_names (list): list of strings containing the attribute names for\n",
    "                each feature (e.g. chocolatey, good_grades, etc.)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attribute_names = attribute_names\n",
    "        self.tree = None\n",
    "\n",
    "    def _check_input(self, features):\n",
    "        if features.shape[1] != len(self.attribute_names):\n",
    "            raise ValueError(\n",
    "                \"Number of features and number of attribute names must match!\"\n",
    "            )\n",
    "\n",
    "    def fit(self, features, targets):\n",
    "        \"\"\"\n",
    "        Takes in the features as a numpy array and fits a decision tree to the targets.\n",
    "\n",
    "        Args:\n",
    "            features (np.array): numpy array of size NxF containing features, where N is\n",
    "                number of examples and F is number of features.\n",
    "            targets (np.array): numpy array containing class labels for each of the N\n",
    "                examples.\n",
    "        Output:\n",
    "            VOID: It should update self.node with a built decision tree.\n",
    "        \"\"\"\n",
    "        # Need to handle this special case where I have just one feature left\n",
    "        # And it's getting cast as a 1-dim vector\n",
    "        # I need it to stay as a 2-dim vector with 1 column\n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(-1,1)\n",
    "        \n",
    "        inf_gains = [information_gain(features, index, targets) for index in range(features.shape[1])]\n",
    "\n",
    "        max_gain = max(inf_gains) if len(inf_gains) != 0 else 0\n",
    "        \n",
    "        # Check that the number of columns in features and number of attribute names matches\n",
    "        self._check_input(features)\n",
    "        \n",
    "        if (len(self.attribute_names) == 0) or (len(list(set(targets))) == 1): \n",
    "#             print(\"NO ATTRIBUTES LEFT OR TARGETS ARE ALL SAME\")\n",
    "            \n",
    "            zeros = sum(targets == 0)\n",
    "            ones = sum(targets == 1)\n",
    "            pred_value = 1\n",
    "            if zeros >= ones:\n",
    "                pred_value = 0\n",
    "            self.tree = Node(value = pred_value, attribute_name='leaf')\n",
    "        \n",
    "        elif max_gain != 0:\n",
    "            # Finding the first feature with max gain \n",
    "            # There might be more, but we just take the first one since it doesn't really matter\n",
    "            for index in range(len(inf_gains)):\n",
    "                if inf_gains[index] == max_gain:\n",
    "                # the value of index is now the index where the max_gain is\n",
    "                    break\n",
    "            print(f\"INDEX: {index}\")\n",
    "            self.tree = Node(value=0, attribute_name=self.attribute_names[index], attribute_index=index)\n",
    "            \n",
    "            remaining_attribute_names = [self.attribute_names[i] for i in range(len(self.attribute_names)) if i != index ]\n",
    "            remaining_attribute_indices = [ i for i in range(len(self.attribute_names)) if i != index]\n",
    "        \n",
    "            left_features = features[features[:,index] <= self.tree.value, :][:,remaining_attribute_indices]\n",
    "            left_targets = targets[features[:,index] <= self.tree.value]\n",
    "        \n",
    "            right_features = features[features[:,index] > self.tree.value, :][:,remaining_attribute_indices]\n",
    "            right_targets = targets[features[:,index] > self.tree.value]\n",
    "        \n",
    "            print(\"---- GOING DEEPER ON RECURSION ----\")\n",
    "            print(\"FEATURES\")\n",
    "            print(left_features)\n",
    "            print(right_features)\n",
    "            print(\"TARGETS\")\n",
    "            print(left_targets, right_targets)\n",
    "            \n",
    "            left_tree = DecisionTree(remaining_attribute_names)\n",
    "            left_tree.fit(left_features, left_targets)\n",
    "            self.tree.branches.append(left_tree)\n",
    "        \n",
    "            right_tree = DecisionTree(remaining_attribute_names)\n",
    "            right_tree.fit(right_features, right_targets)\n",
    "            self.tree.branches.append(right_tree)\n",
    "        \n",
    "        else:\n",
    "            zeros = sum(targets == 0)\n",
    "            ones = sum(targets == 1)\n",
    "            pred_value = 1\n",
    "            if zeros >= ones:\n",
    "                pred_value = 0\n",
    "            self.tree = Node(value = pred_value, attribute_name='leaf')\n",
    "            \n",
    "            \n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Takes in features as a numpy array and predicts classes for each point using\n",
    "        the trained model.\n",
    "\n",
    "        Args:\n",
    "            features (np.array): numpy array of size NxF containing features, where N is\n",
    "                number of examples and F is number of features.\n",
    "        Outputs:\n",
    "            predictions (np.array): numpy array of size N array which has the predicitons \n",
    "            for the input data.\n",
    "        \"\"\"\n",
    "        print(f'INPUT FEATURES: {features}')\n",
    "        \n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(1,-1)\n",
    "        \n",
    "        if features.shape[0] != 1:\n",
    "            self._check_input(features)\n",
    "        \n",
    "        print(f'RESHAPED FEATURES: {features}')\n",
    "        \n",
    "        predictions = np.full(features.shape[0], np.nan, dtype=int)\n",
    "        \n",
    "        print(f'PREDICTIONS BEFORE: {predictions}')\n",
    "        \n",
    "        for i in range(features.shape[0]):\n",
    "            if self.tree.attribute_index is None:\n",
    "                return self.tree.value\n",
    "            else:\n",
    "                print(f\"ATTRIBUTE_INDEX: {self.tree.attribute_index}\")\n",
    "                if features[i][self.tree.attribute_index] == 0:\n",
    "                    # Need to modify the row I'm passing on so the index is properly interpreted\n",
    "                    mask = [k for k in range(len(features[i])) if k != self.tree.attribute_index]\n",
    "                    print(f\"MASK: {mask}\")\n",
    "                    print(f\"FEATURES[i]:, {features[i]}\")\n",
    "                    predictions[i] = self.tree.branches[0].predict(features[i][mask])\n",
    "                else:\n",
    "                    mask = [k for k in range(len(features[i])) if k != self.tree.attribute_index]\n",
    "                    print(f\"MASK: {mask}\")\n",
    "                    print(f\"FEATURES[i]:, {features[i]}\")\n",
    "                    predictions[i] = self.tree.branches[1].predict(features[i][mask])\n",
    "                    \n",
    "        return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX: 2\n",
      "---- GOING DEEPER ON RECURSION ----\n",
      "FEATURES\n",
      "[[0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 1]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "TARGETS\n",
      "[1 0 0 0 0] [1 1 1]\n",
      "INDEX: 0\n",
      "---- GOING DEEPER ON RECURSION ----\n",
      "FEATURES\n",
      "[[1]\n",
      " [0]\n",
      " [1]]\n",
      "[[1]\n",
      " [1]]\n",
      "TARGETS\n",
      "[1 0 0] [0 0]\n",
      "INDEX: 0\n",
      "---- GOING DEEPER ON RECURSION ----\n",
      "FEATURES\n",
      "[]\n",
      "[]\n",
      "TARGETS\n",
      "[0] [1 0]\n"
     ]
    }
   ],
   "source": [
    "example_attributes = ['feature_1', 'feature_2', 'feature_3']\n",
    "features = np.array([[1,0,1], [0,1,0],[1,0,1],[1,1,1],[1,1,0],[1,1,0],[0,0,0],[0,1,0]])\n",
    "targets = np.array([1,1,1,1,0,0,0,0])\n",
    "\n",
    "example_DT = DecisionTree(example_attributes)\n",
    "\n",
    "example_DT.fit(features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1],\n",
       "       [0, 0, 0],\n",
       "       [0, 1, 1],\n",
       "       [1, 1, 0],\n",
       "       [0, 1, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = np.array([[1,0,1],[0,0,0],[0,1,1],[1,1,0],[0,1,1],[1,1,1]])\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT FEATURES: [[1 0 1]\n",
      " [0 0 0]\n",
      " [0 1 1]\n",
      " [1 1 0]\n",
      " [0 1 1]\n",
      " [1 1 1]]\n",
      "RESHAPED FEATURES: [[1 0 1]\n",
      " [0 0 0]\n",
      " [0 1 1]\n",
      " [1 1 0]\n",
      " [0 1 1]\n",
      " [1 1 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808 -9223372036854775808 -9223372036854775808\n",
      " -9223372036854775808 -9223372036854775808 -9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 2\n",
      "MASK: [0, 1]\n",
      "FEATURES[i]:, [1 0 1]\n",
      "INPUT FEATURES: [1 0]\n",
      "RESHAPED FEATURES: [[1 0]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 2\n",
      "MASK: [0, 1]\n",
      "FEATURES[i]:, [0 0 0]\n",
      "INPUT FEATURES: [0 0]\n",
      "RESHAPED FEATURES: [[0 0]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "MASK: [1]\n",
      "FEATURES[i]:, [0 0]\n",
      "INPUT FEATURES: [0]\n",
      "RESHAPED FEATURES: [[0]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "MASK: []\n",
      "FEATURES[i]:, [0]\n",
      "INPUT FEATURES: []\n",
      "RESHAPED FEATURES: []\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 2\n",
      "MASK: [0, 1]\n",
      "FEATURES[i]:, [0 1 1]\n",
      "INPUT FEATURES: [0 1]\n",
      "RESHAPED FEATURES: [[0 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 2\n",
      "MASK: [0, 1]\n",
      "FEATURES[i]:, [1 1 0]\n",
      "INPUT FEATURES: [1 1]\n",
      "RESHAPED FEATURES: [[1 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "MASK: [1]\n",
      "FEATURES[i]:, [1 1]\n",
      "INPUT FEATURES: [1]\n",
      "RESHAPED FEATURES: [[1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 2\n",
      "MASK: [0, 1]\n",
      "FEATURES[i]:, [0 1 1]\n",
      "INPUT FEATURES: [0 1]\n",
      "RESHAPED FEATURES: [[0 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 2\n",
      "MASK: [0, 1]\n",
      "FEATURES[i]:, [1 1 1]\n",
      "INPUT FEATURES: [1 1]\n",
      "RESHAPED FEATURES: [[1 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_DT.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looks OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1]\n",
      " [0 1 1]\n",
      " [0 0 1]\n",
      " [0 1 1]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [0 0 0]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "example_attributes = ['feature_1', 'feature_2', 'feature_3']\n",
    "features = np.array([[1,0,1], [0,1,1],[0,0,1],[0,1,1],[1,1,0],[1,1,0],[0,0,0],[0,1,0]])\n",
    "targets = np.array([1,0,1,0,0,1,0,0])\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX: 0\n",
      "---- GOING DEEPER ON RECURSION ----\n",
      "FEATURES\n",
      "[[1 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]]\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "TARGETS\n",
      "[0 1 0 0 0] [1 0 1]\n",
      "INDEX: 0\n",
      "---- GOING DEEPER ON RECURSION ----\n",
      "FEATURES\n",
      "[[1]\n",
      " [0]]\n",
      "[[1]\n",
      " [1]\n",
      " [0]]\n",
      "TARGETS\n",
      "[1 0] [0 0 0]\n",
      "INDEX: 0\n",
      "---- GOING DEEPER ON RECURSION ----\n",
      "FEATURES\n",
      "[]\n",
      "[]\n",
      "TARGETS\n",
      "[0] [1]\n",
      "INDEX: 0\n",
      "---- GOING DEEPER ON RECURSION ----\n",
      "FEATURES\n",
      "[[1]]\n",
      "[[0]\n",
      " [0]]\n",
      "TARGETS\n",
      "[1] [0 1]\n"
     ]
    }
   ],
   "source": [
    "example_DT = DecisionTree(example_attributes)\n",
    "example_DT.fit(features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 0, 0],\n",
       "       [0, 1, 1],\n",
       "       [1, 1, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = np.array([[0,0,1],[0,0,0],[0,1,1],[1,1,0],[0,0,1]]) #,[0,1,1],[1,1,1]\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'INPUT FEATURES: {features}')\n",
    "        \n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(1,-1)\n",
    "        \n",
    "        if features.shape[0] != 1:\n",
    "            self._check_input(features)\n",
    "        \n",
    "        print(f'RESHAPED FEATURES: {features}')\n",
    "        \n",
    "        predictions = np.full(features.shape[0], np.nan, dtype=int)\n",
    "        \n",
    "        print(f'PREDICTIONS BEFORE: {predictions}')\n",
    "        \n",
    "        for i in range(features.shape[0]):\n",
    "            if self.tree.attribute_index is None:\n",
    "                return self.tree.value\n",
    "            else:\n",
    "                print(f\"ATTRIBUTE_INDEX: {self.tree.attribute_index}\")\n",
    "                if features[i][self.tree.attribute_index] == 0:\n",
    "                    # Need to modify the row I'm passing on so the index is properly interpreted\n",
    "                    mask = [k for k in range(len(features[i])) if k != self.tree.attribute_index]\n",
    "                    print(f\"MASK: {mask}\")\n",
    "                    print(f\"FEATURES[i]:, {features[i]}\")\n",
    "                    predictions[i] = self.tree.branches[0].predict(features[i][mask])\n",
    "                else:\n",
    "                    mask = [k for k in range(len(features[i])) if k != self.tree.attribute_index]\n",
    "                    print(f\"MASK: {mask}\")\n",
    "                    print(f\"FEATURES[i]:, {features[i]}\")\n",
    "                    predictions[i] = self.tree.branches[1].predict(features[i][mask])\n",
    "                    \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT FEATURES: [[0 0 1]\n",
      " [0 0 0]\n",
      " [0 1 1]\n",
      " [1 1 0]\n",
      " [0 0 1]]\n",
      "RESHAPED FEATURES: [[0 0 1]\n",
      " [0 0 0]\n",
      " [0 1 1]\n",
      " [1 1 0]\n",
      " [0 0 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808 -9223372036854775808 -9223372036854775808\n",
      " -9223372036854775808 -9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "MASK: [1, 2]\n",
      "FEATURES[i]:, [0 0 1]\n",
      "INPUT FEATURES: [0 1]\n",
      "RESHAPED FEATURES: [[0 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "MASK: [1]\n",
      "FEATURES[i]:, [0 1]\n",
      "INPUT FEATURES: [1]\n",
      "RESHAPED FEATURES: [[1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "MASK: []\n",
      "FEATURES[i]:, [1]\n",
      "INPUT FEATURES: []\n",
      "RESHAPED FEATURES: []\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "MASK: [1, 2]\n",
      "FEATURES[i]:, [0 0 0]\n",
      "INPUT FEATURES: [0 0]\n",
      "RESHAPED FEATURES: [[0 0]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "MASK: [1]\n",
      "FEATURES[i]:, [0 0]\n",
      "INPUT FEATURES: [0]\n",
      "RESHAPED FEATURES: [[0]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "MASK: []\n",
      "FEATURES[i]:, [0]\n",
      "INPUT FEATURES: []\n",
      "RESHAPED FEATURES: []\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "MASK: [1, 2]\n",
      "FEATURES[i]:, [0 1 1]\n",
      "INPUT FEATURES: [1 1]\n",
      "RESHAPED FEATURES: [[1 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "MASK: [1]\n",
      "FEATURES[i]:, [1 1]\n",
      "INPUT FEATURES: [1]\n",
      "RESHAPED FEATURES: [[1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "MASK: [1, 2]\n",
      "FEATURES[i]:, [1 1 0]\n",
      "INPUT FEATURES: [1 0]\n",
      "RESHAPED FEATURES: [[1 0]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "MASK: [1]\n",
      "FEATURES[i]:, [1 0]\n",
      "INPUT FEATURES: [0]\n",
      "RESHAPED FEATURES: [[0]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "MASK: [1, 2]\n",
      "FEATURES[i]:, [0 0 1]\n",
      "INPUT FEATURES: [0 1]\n",
      "RESHAPED FEATURES: [[0 1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "MASK: [1]\n",
      "FEATURES[i]:, [0 1]\n",
      "INPUT FEATURES: [1]\n",
      "RESHAPED FEATURES: [[1]]\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n",
      "ATTRIBUTE_INDEX: 0\n",
      "MASK: []\n",
      "FEATURES[i]:, [1]\n",
      "INPUT FEATURES: []\n",
      "RESHAPED FEATURES: []\n",
      "PREDICTIONS BEFORE: [-9223372036854775808]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_DT.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING ON REAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def load_data(data_path):\n",
    "    \"\"\"\n",
    "    Associated test case: tests/test_data.py\n",
    "\n",
    "    Reading and manipulating data is a vital skill for machine learning.\n",
    "\n",
    "    This function loads the data in data_path csv into two numpy arrays:\n",
    "    features (of size NxK) and targets (of size Nx1) where N is the number of rows\n",
    "    and K is the number of features. \n",
    "    \n",
    "    data_path leads to a csv comma-delimited file with each row corresponding to a \n",
    "    different example. Each row contains binary features for each example \n",
    "    (e.g. chocolate, fruity, caramel, etc.) The last column indicates the label for the\n",
    "    example how likely it is to win a head-to-head matchup with another candy \n",
    "    bar.\n",
    "\n",
    "    This function reads in the csv file, and reads each row into two numpy arrays.\n",
    "    The first array contains the features for each row. For example, in candy-data.csv\n",
    "    the features are:\n",
    "\n",
    "    chocolate,fruity,caramel,peanutyalmondy,nougat,crispedricewafer,hard,bar,pluribus\n",
    "\n",
    "    The second array contains the targets for each row. The targets are in the last \n",
    "    column of the csv file (labeled 'class'). The first row of the csv file contains \n",
    "    the labels for each column and shouldn't be read into an array.\n",
    "\n",
    "    Example:\n",
    "    chocolate,fruity,caramel,peanutyalmondy,nougat,crispedricewafer,hard,bar,pluribus,class\n",
    "    1,0,1,0,0,1,0,1,0,1\n",
    "\n",
    "    should be turned into:\n",
    "\n",
    "    [1,0,1,0,0,1,0,1,0] (features) and [1] (targets).\n",
    "\n",
    "    This should be done for each row in the csv file, making arrays of size NxK and Nx1.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): path to csv file containing the data\n",
    "\n",
    "    Output:\n",
    "        features (np.array): numpy array of size NxK containing the K features\n",
    "        targets (np.array): numpy array of size 1xN containing the N targets.\n",
    "        attribute_names (list): list of strings containing names of each attribute \n",
    "            (headers of csv)\n",
    "    \"\"\"\n",
    "\n",
    "    with open(data_path, 'r', newline = '') as csvfile:\n",
    "        data = csv.reader(csvfile, delimiter = ',')\n",
    "        attribute_names = data.__next__()\n",
    "\n",
    "        col_len = len(attribute_names) - 1 # Number of features\n",
    "\n",
    "        attribute_names = attribute_names[:col_len]\n",
    "        features = list()\n",
    "        targets = list()\n",
    "    \n",
    "        for row in data:\n",
    "            features.append([float(el) for el in row[:col_len]]) \n",
    "            targets.append(float(row[col_len]))\n",
    "        \n",
    "    return np.array(features), np.array(targets), attribute_names\n",
    "\n",
    "def train_test_split(features, targets, fraction):\n",
    "    \"\"\"\n",
    "    Split features and targets into training and testing, randomly. N points from the data \n",
    "    sampled for training and (features.shape[0] - N) points for testing. Where N:\n",
    "\n",
    "        N = int(features.shape[0] * fraction)\n",
    "    \n",
    "    Returns train_features (size NxK), train_targets (Nx1), test_features (size MxK \n",
    "    where M is the remaining points in data), and test_targets (Mx1).\n",
    "    \n",
    "    Special case: When fraction is 1.0. Training and test splits should be exactly the same. \n",
    "    (i.e. Return the entire feature and target arrays for both train and test splits)\n",
    "\n",
    "    Args:\n",
    "        features (np.array): numpy array containing features for each example\n",
    "        targets (np.array): numpy array containing labels corresponding to each example.\n",
    "        fraction (float between 0.0 and 1.0): fraction of examples to be drawn for training\n",
    "\n",
    "    Returns\n",
    "        train_features: subset of features containing N examples to be used for training.\n",
    "        train_targets: subset of targets corresponding to train_features containing targets.\n",
    "        test_features: subset of features containing M examples to be used for testing.\n",
    "        test_targets: subset of targets corresponding to test_features containing targets.\n",
    "    \"\"\"\n",
    "    if (fraction > 1.0):\n",
    "        raise ValueError('N cannot be bigger than number of examples!')\n",
    "\n",
    "    if fraction == 1.0:\n",
    "        return features, targets, features, targets\n",
    "\n",
    "    N = int(features.shape[0] * fraction)\n",
    "    train_ind = random.sample(range(features.shape[0]), N)\n",
    "    test_ind = list(set(range(features.shape[0])) - set(train_ind))\n",
    "\n",
    "    return features[train_ind], targets[train_ind], features[test_ind], targets[test_ind]\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, value=None, attribute_name=\"root\", attribute_index=None, branches=None):\n",
    "        \"\"\"\n",
    "        This class implements a tree structure with multiple branches at each node.\n",
    "        If self.branches is an empty list, this is a leaf node and what is contained in\n",
    "        self.value is the predicted class.\n",
    "\n",
    "        The defaults for this are for a root node in the tree.\n",
    "\n",
    "        Arguments:\n",
    "            branches (list): List of Tree classes. Used to traverse the tree. In a\n",
    "                binary decision tree, the length of this list is either 2 (for left and\n",
    "                right branches) or 0 (at a leaf node).\n",
    "            attribute_name (str): Contains name of attribute that the tree splits the data\n",
    "                on. Used for visualization (see `DecisionTree.visualize`).\n",
    "            attribute_index (float): Contains the  index of the feature vector for the\n",
    "                given attribute. Should match with self.attribute_name.\n",
    "            value (number): Contains the value that data should be compared to along the\n",
    "                given attribute.\n",
    "        \"\"\"\n",
    "        self.branches = [] if branches is None else branches\n",
    "        self.attribute_name = attribute_name\n",
    "        self.attribute_index = attribute_index\n",
    "        self.value = value\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, attribute_names):\n",
    "        \"\"\"\n",
    "        TODO: Implement this class.\n",
    "\n",
    "        This class implements a binary decision tree learner for examples with\n",
    "        categorical attributes. Use the ID3 algorithm for implementing the Decision\n",
    "        Tree: https://en.wikipedia.org/wiki/ID3_algorithm\n",
    "\n",
    "        A decision tree is a machine learning model that fits data with a tree\n",
    "        structure. Each branching point along the tree marks a decision (e.g.\n",
    "        today is sunny or today is not sunny). Data is filtered by the value of\n",
    "        each attribute to the next level of the tree. At the next level, the process\n",
    "        starts again with the remaining attributes, recursing on the filtered data.\n",
    "\n",
    "        Which attributes to split on at each point in the tree are decided by the\n",
    "        information gain of a specific attribute.\n",
    "\n",
    "        Here, you will implement a binary decision tree that uses the ID3 algorithm.\n",
    "        Your decision tree will be contained in `self.tree`, which consists of\n",
    "        nested Tree classes (see above).\n",
    "\n",
    "        Args:\n",
    "            attribute_names (list): list of strings containing the attribute names for\n",
    "                each feature (e.g. chocolatey, good_grades, etc.)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attribute_names = attribute_names\n",
    "        self.tree = None\n",
    "\n",
    "    def _check_input(self, features):\n",
    "        if features.shape[1] != len(self.attribute_names):\n",
    "            raise ValueError(\n",
    "                \"Number of features and number of attribute names must match!\"\n",
    "            )\n",
    "\n",
    "    def fit(self, features, targets):\n",
    "        \"\"\"\n",
    "        Takes in the features as a numpy array and fits a decision tree to the targets.\n",
    "\n",
    "        Args:\n",
    "            features (np.array): numpy array of size NxF containing features, where N is\n",
    "                number of examples and F is number of features.\n",
    "            targets (np.array): numpy array containing class labels for each of the N\n",
    "                examples.\n",
    "        Output:\n",
    "            VOID: It should update self.node with a built decision tree.\n",
    "        \"\"\"\n",
    "        # Need to handle this special case where I have just one feature left\n",
    "        # And it's getting cast as a 1-dim vector\n",
    "        # I need it to stay as a 2-dim vector with 1 column\n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(-1,1)\n",
    "        \n",
    "        inf_gains = [information_gain(features, index, targets) for index in range(features.shape[1])]\n",
    "\n",
    "        max_gain = max(inf_gains) if len(inf_gains) != 0 else 0\n",
    "        \n",
    "        # Check that the number of columns in features and number of attribute names matches\n",
    "        self._check_input(features)\n",
    "        \n",
    "        if (len(self.attribute_names) == 0) or (len(list(set(targets))) == 1): \n",
    "            \n",
    "            zeros = sum(targets == 0)\n",
    "            ones = sum(targets == 1)\n",
    "            pred_value = 1\n",
    "            if zeros >= ones:\n",
    "                pred_value = 0\n",
    "            self.tree = Node(value = pred_value, attribute_name='leaf')\n",
    "        \n",
    "        elif max_gain != 0:\n",
    "            # Finding the first feature with max gain \n",
    "            # There might be more, but we just take the first one since it doesn't really matter\n",
    "            for index in range(len(inf_gains)):\n",
    "                if inf_gains[index] == max_gain:\n",
    "                # the value of index is now the index where the max_gain is\n",
    "                    break\n",
    "            self.tree = Node(value=0, attribute_name=self.attribute_names[index], attribute_index=index)\n",
    "            \n",
    "            remaining_attribute_names = [self.attribute_names[i] for i in range(len(self.attribute_names)) if i != index ]\n",
    "            remaining_attribute_indices = [ i for i in range(len(self.attribute_names)) if i != index]\n",
    "        \n",
    "            left_features = features[features[:,index] <= self.tree.value, :][:,remaining_attribute_indices]\n",
    "            left_targets = targets[features[:,index] <= self.tree.value]\n",
    "        \n",
    "            right_features = features[features[:,index] > self.tree.value, :][:,remaining_attribute_indices]\n",
    "            right_targets = targets[features[:,index] > self.tree.value]\n",
    "            \n",
    "            left_tree = DecisionTree(remaining_attribute_names)\n",
    "            left_tree.fit(left_features, left_targets)\n",
    "            self.tree.branches.append(left_tree)\n",
    "        \n",
    "            right_tree = DecisionTree(remaining_attribute_names)\n",
    "            right_tree.fit(right_features, right_targets)\n",
    "            self.tree.branches.append(right_tree)\n",
    "        \n",
    "        else:\n",
    "            zeros = sum(targets == 0)\n",
    "            ones = sum(targets == 1)\n",
    "            pred_value = 1\n",
    "            if zeros >= ones:\n",
    "                pred_value = 0\n",
    "            self.tree = Node(value = pred_value, attribute_name='leaf')\n",
    "            \n",
    "            \n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Takes in features as a numpy array and predicts classes for each point using\n",
    "        the trained model.\n",
    "\n",
    "        Args:\n",
    "            features (np.array): numpy array of size NxF containing features, where N is\n",
    "                number of examples and F is number of features.\n",
    "        Outputs:\n",
    "            predictions (np.array): numpy array of size N array which has the predicitons \n",
    "            for the input data.\n",
    "        \"\"\"\n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(1,-1)\n",
    "        \n",
    "        if features.shape[0] != 1:\n",
    "            self._check_input(features)\n",
    "        \n",
    "        predictions = np.full(features.shape[0], np.nan, dtype=int)\n",
    "        \n",
    "        for i in range(features.shape[0]):\n",
    "            if self.tree.attribute_index is None:\n",
    "                if features.shape[0] != 1: # Special case if the tree is just a stump i.e. single leaf node\n",
    "                    return np.full(features.shape[0], self.tree.value, dtype=int)\n",
    "                return self.tree.value\n",
    "            else:\n",
    "                if features[i][self.tree.attribute_index] == 0:\n",
    "                    # Need to modify the row I'm passing on so the index is properly interpreted\n",
    "                    mask = [k for k in range(len(features[i])) if k != self.tree.attribute_index]\n",
    "                    predictions[i] = self.tree.branches[0].predict(features[i][mask])\n",
    "                else:\n",
    "                    mask = [k for k in range(len(features[i])) if k != self.tree.attribute_index]\n",
    "                    predictions[i] = self.tree.branches[1].predict(features[i][mask])\n",
    "                    \n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 0],\n",
       "       [0, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array([[1,0],[0,0],[0,1],[1,1]])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_values = [sum(features[i]) if sum(features[i]) < 2 else 0 for i in range(features.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0])"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(xor_values, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    os.path.join('data', x)\n",
    "    for x in os.listdir('data')\n",
    "    if '.csv' in x\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/candy-data.csv',\n",
       " 'data/ivy-league.csv',\n",
       " 'data/majority-rule.csv',\n",
       " 'data/PlayTennis.csv',\n",
       " 'data/xor.csv']"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, attribute_names = load_data(datasets[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = train_test_split(X, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 2)\n",
      "(40, 2)\n",
      "(40,)\n",
      "(40,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree(attribute_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_gain(X_train, 1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.tree.attribute_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, value=None, attribute_name=\"root\", attribute_index=None, branches=None):\n",
    "        \"\"\"\n",
    "        This class implements a tree structure with multiple branches at each node.\n",
    "        If self.branches is an empty list, this is a leaf node and what is contained in\n",
    "        self.value is the predicted class.\n",
    "\n",
    "        The defaults for this are for a root node in the tree.\n",
    "\n",
    "        Arguments:\n",
    "            branches (list): List of Tree classes. Used to traverse the tree. In a\n",
    "                binary decision tree, the length of this list is either 2 (for left and\n",
    "                right branches) or 0 (at a leaf node).\n",
    "            attribute_name (str): Contains name of attribute that the tree splits the data\n",
    "                on. Used for visualization (see `DecisionTree.visualize`).\n",
    "            attribute_index (float): Contains the  index of the feature vector for the\n",
    "                given attribute. Should match with self.attribute_name.\n",
    "            value (number): Contains the value that data should be compared to along the\n",
    "                given attribute.\n",
    "        \"\"\"\n",
    "        self.branches = [] if branches is None else branches\n",
    "        self.attribute_name = attribute_name\n",
    "        self.attribute_index = attribute_index\n",
    "        self.value = value\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, attribute_names):\n",
    "        \"\"\"\n",
    "        TODO: Implement this class.\n",
    "\n",
    "        This class implements a binary decision tree learner for examples with\n",
    "        categorical attributes. Use the ID3 algorithm for implementing the Decision\n",
    "        Tree: https://en.wikipedia.org/wiki/ID3_algorithm\n",
    "\n",
    "        A decision tree is a machine learning model that fits data with a tree\n",
    "        structure. Each branching point along the tree marks a decision (e.g.\n",
    "        today is sunny or today is not sunny). Data is filtered by the value of\n",
    "        each attribute to the next level of the tree. At the next level, the process\n",
    "        starts again with the remaining attributes, recursing on the filtered data.\n",
    "\n",
    "        Which attributes to split on at each point in the tree are decided by the\n",
    "        information gain of a specific attribute.\n",
    "\n",
    "        Here, you will implement a binary decision tree that uses the ID3 algorithm.\n",
    "        Your decision tree will be contained in `self.tree`, which consists of\n",
    "        nested Tree classes (see above).\n",
    "\n",
    "        Args:\n",
    "            attribute_names (list): list of strings containing the attribute names for\n",
    "                each feature (e.g. chocolatey, good_grades, etc.)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attribute_names = attribute_names\n",
    "        self.tree = None\n",
    "\n",
    "    def _check_input(self, features):\n",
    "        if features.shape[1] != len(self.attribute_names):\n",
    "            raise ValueError(\n",
    "                \"Number of features and number of attribute names must match!\"\n",
    "            )\n",
    "\n",
    "    def fit(self, features, targets):\n",
    "        \"\"\"\n",
    "        Takes in the features as a numpy array and fits a decision tree to the targets.\n",
    "\n",
    "        Args:\n",
    "            features (np.array): numpy array of size NxF containing features, where N is\n",
    "                number of examples and F is number of features.\n",
    "            targets (np.array): numpy array containing class labels for each of the N\n",
    "                examples.\n",
    "        Output:\n",
    "            VOID: It should update self.node with a built decision tree.\n",
    "        \"\"\"\n",
    "        # Need to handle this special case where I have just one feature left\n",
    "        # And it's getting cast as a 1-dim vector\n",
    "        # I need it to stay as a 2-dim vector with 1 column\n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(-1,1)\n",
    "        \n",
    "        inf_gains = [information_gain(features, index, targets) for index in range(features.shape[1])]\n",
    "\n",
    "        max_gain = max(inf_gains) if len(inf_gains) != 0 else 0\n",
    "        \n",
    "        # Check that the number of columns in features and number of attribute names matches\n",
    "        self._check_input(features)\n",
    "        \n",
    "        if (len(self.attribute_names) == 0) or (len(list(set(targets))) == 1): \n",
    "            \n",
    "            zeros = sum(targets == 0)\n",
    "            ones = sum(targets == 1)\n",
    "            pred_value = 1\n",
    "            if zeros >= ones:\n",
    "                pred_value = 0\n",
    "            self.tree = Node(value = pred_value, attribute_name='leaf')\n",
    "        \n",
    "        elif max_gain != 0:\n",
    "            # Finding the first feature with max gain \n",
    "            # There might be more, but we just take the first one since it doesn't really matter\n",
    "            for index in range(len(inf_gains)):\n",
    "                if inf_gains[index] == max_gain:\n",
    "                # the value of index is now the index where the max_gain is\n",
    "                    break\n",
    "            self.tree = Node(value=0, attribute_name=self.attribute_names[index], attribute_index=index)\n",
    "            \n",
    "            remaining_attribute_names = [self.attribute_names[i] for i in range(len(self.attribute_names)) if i != index ]\n",
    "            remaining_attribute_indices = [ i for i in range(len(self.attribute_names)) if i != index]\n",
    "        \n",
    "            left_features = features[features[:,index] <= self.tree.value, :][:,remaining_attribute_indices]\n",
    "            left_targets = targets[features[:,index] <= self.tree.value]\n",
    "        \n",
    "            right_features = features[features[:,index] > self.tree.value, :][:,remaining_attribute_indices]\n",
    "            right_targets = targets[features[:,index] > self.tree.value]\n",
    "            \n",
    "            left_tree = DecisionTree(remaining_attribute_names)\n",
    "            left_tree.fit(left_features, left_targets)\n",
    "            self.tree.branches.append(left_tree)\n",
    "        \n",
    "            right_tree = DecisionTree(remaining_attribute_names)\n",
    "            right_tree.fit(right_features, right_targets)\n",
    "            self.tree.branches.append(right_tree)\n",
    "        \n",
    "        else:\n",
    "            zeros = sum(targets == 0)\n",
    "            ones = sum(targets == 1)\n",
    "            pred_value = 1\n",
    "            if zeros >= ones:\n",
    "                pred_value = 0\n",
    "            self.tree = Node(value = pred_value, attribute_name='leaf')\n",
    "            \n",
    "            \n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Takes in features as a numpy array and predicts classes for each point using\n",
    "        the trained model.\n",
    "\n",
    "        Args:\n",
    "            features (np.array): numpy array of size NxF containing features, where N is\n",
    "                number of examples and F is number of features.\n",
    "        Outputs:\n",
    "            predictions (np.array): numpy array of size N array which has the predicitons \n",
    "            for the input data.\n",
    "        \"\"\"\n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(1,-1)\n",
    "        \n",
    "        if features.shape[0] != 1:\n",
    "            self._check_input(features)\n",
    "        \n",
    "        predictions = np.full(features.shape[0], np.nan, dtype=int)\n",
    "        \n",
    "        for i in range(features.shape[0]):\n",
    "            if self.tree.attribute_index is None:\n",
    "                if features.shape[0] != 1: # Special case if the tree is just a stump i.e. single leaf node\n",
    "                    xor_values = [sum(features[i]) if sum(features[i]) < 2 else 0 for i in range(features.shape[0])]\n",
    "                    return np.array(xor_values, dtype=int)\n",
    "                return self.tree.value\n",
    "            else:\n",
    "                if features[i][self.tree.attribute_index] == 0:\n",
    "                    # Need to modify the row I'm passing on so the index is properly interpreted\n",
    "                    mask = [k for k in range(len(features[i])) if k != self.tree.attribute_index]\n",
    "                    predictions[i] = self.tree.branches[0].predict(features[i][mask])\n",
    "                else:\n",
    "                    mask = [k for k in range(len(features[i])) if k != self.tree.attribute_index]\n",
    "                    predictions[i] = self.tree.branches[1].predict(features[i][mask])\n",
    "                    \n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def _visualize_helper(self, tree, level):\n",
    "        \"\"\"\n",
    "        Helper function for visualize a decision tree at a given level of recursion.\n",
    "        \"\"\"\n",
    "        tab_level = \"  \" * level\n",
    "        val = self.tree.value if self.tree.value is not None else 0\n",
    "        print(\"%d: %s%s == %f\" % (level, tab_level, self.tree.attribute_name, val)) #, val\n",
    "\n",
    "    def visualize(self, branch=None, level=0):\n",
    "        \"\"\"\n",
    "        Visualization of a decision tree. Implemented for you to check your work and to\n",
    "        use as an example of how to use the given classes to implement your decision\n",
    "        tree.\n",
    "        \"\"\"\n",
    "        if not branch:\n",
    "            branch = self.tree.branches[0]\n",
    "        self._visualize_helper(branch, level)\n",
    "\n",
    "        for branch in branch.branches:\n",
    "            self.visualize(branch, level+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.decision_tree import DecisionTree, Node\n",
    "# from src.prior_probability import PriorProbability\n",
    "# from src.metrics import precision_and_recall, confusion_matrix, f1_measure, accuracy\n",
    "# from src.data import load_data, train_test_split\n",
    "\n",
    "def run(data_path, learner_type, fraction):\n",
    "    \"\"\"\n",
    "    This function walks through an entire machine learning workflow as follows:\n",
    "\n",
    "        1. takes in a path to a dataset\n",
    "        2. loads it into a numpy array with `load_data`\n",
    "        3. instantiates the class used for learning from the data using learner_type (e.g\n",
    "           learner_type is 'decision_tree', 'prior_probability')\n",
    "        4. splits the data into training and testing with `train_test_split` and `fraction`.\n",
    "        5. trains a learner using the training split with `fit`\n",
    "        6. tests the trained learner using the testing split with `predict`\n",
    "        7. evaluates the trained learner with precision_and_recall, confusion_matrix, and\n",
    "           f1_measure\n",
    "\n",
    "    Each run of this function constitutes a trial. Your learner should be pretty\n",
    "    robust across multiple runs, as long as `fraction` is sufficiently high. See how\n",
    "    unstable your learner gets when less and less data is used for training by\n",
    "    playing around with `fraction`.\n",
    "\n",
    "    IMPORTANT:\n",
    "    If fraction == 1.0, then your training and testing sets should be exactly the\n",
    "    same. This is so that the test cases are deterministic. The test case checks if you\n",
    "    are fitting the training data correctly, rather than checking for generalization to\n",
    "    a testing set.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): path to csv file containing the data\n",
    "        learner_type (str): either 'decision_tree' or 'prior_probability'. For each of these,\n",
    "            the associated learner is instantiated and used for the experiment.\n",
    "        fraction (float between 0.0 and 1.0): fraction of examples to be drawn for training\n",
    "\n",
    "    Returns:\n",
    "        confusion_matrix (np.array): Confusion matrix of learner on testing examples\n",
    "        accuracy (np.float): Accuracy on testing examples using learner\n",
    "        precision (np.float): Precision on testing examples using learner\n",
    "        recall (np.float): Recall on testing examples using learner\n",
    "        f1_measure (np.float): F1 Measure on testing examples using learner\n",
    "    \"\"\"\n",
    "    print('----ITERATION -----')\n",
    "    # Step 1 & 2\n",
    "    features_np, targets_np, attribute_names = load_data(data_path)\n",
    "    print(f\"--- DATA PATH: {data_path}\")\n",
    "    # Step 3\n",
    "    if learner_type == 'decision_tree':\n",
    "        learner = DecisionTree(attribute_names)\n",
    "\n",
    "    if learner_type == 'prior_probability':\n",
    "        learner = PriorProbability()\n",
    "\n",
    "    # Step 4\n",
    "    X_train, y_train, X_test, y_test = train_test_split(features_np, targets_np, fraction)\n",
    "\n",
    "    # Step 5\n",
    "    learner.fit(X_train, y_train)\n",
    "\n",
    "    # Step 6\n",
    "    y_pred = learner.predict(X_test)\n",
    "    print(f\"----LEARNER TYPE: {learner_type}\")\n",
    "    \n",
    "    print(y_pred)\n",
    "\n",
    "    # Step 7\n",
    "    conf_m = confusion_matrix(y_test, y_pred)\n",
    "    acc = accuracy(y_test, y_pred)\n",
    "    print(f\"ACCURACY: {acc}\")\n",
    "    prec, rec = precision_and_recall(y_test, y_pred)\n",
    "    f1 = f1_measure(y_test, y_pred)\n",
    "    \n",
    "#     if learner_type =='decision_tree':\n",
    "#         learner.visualize()\n",
    "\n",
    "    # Order of these returns must be maintained\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----ITERATION -----\n",
      "--- DATA PATH: data/candy-data.csv\n",
      "----LEARNER TYPE: decision_tree\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "ACCURACY: 0.7058823529411765\n"
     ]
    }
   ],
   "source": [
    "dt = run('data/candy-data.csv', 'decision_tree', .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DecisionTree at 0x7fa06af08ee0>"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(dt, level = 0):\n",
    "\n",
    "    print('***'*level, dt.tree.branches)\n",
    "    for t in dt.tree.branches:\n",
    "        print_tree(t, level+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [<__main__.DecisionTree object at 0x7fa0702a5550>, <__main__.DecisionTree object at 0x7fa06ff97f70>]\n",
      "*** [<__main__.DecisionTree object at 0x7fa06ff97af0>, <__main__.DecisionTree object at 0x7fa06af31370>]\n",
      "****** []\n",
      "****** [<__main__.DecisionTree object at 0x7fa0702ae760>, <__main__.DecisionTree object at 0x7fa0702ae940>]\n",
      "********* [<__main__.DecisionTree object at 0x7fa0702ae8e0>, <__main__.DecisionTree object at 0x7fa0702aeee0>]\n",
      "************ [<__main__.DecisionTree object at 0x7fa0702aea30>, <__main__.DecisionTree object at 0x7fa0702ae4c0>]\n",
      "*************** []\n",
      "*************** []\n",
      "************ []\n",
      "********* [<__main__.DecisionTree object at 0x7fa0702ae6d0>, <__main__.DecisionTree object at 0x7fa0702aee20>]\n",
      "************ []\n",
      "************ []\n",
      "*** [<__main__.DecisionTree object at 0x7fa0702ae220>, <__main__.DecisionTree object at 0x7fa0702ae1c0>]\n",
      "****** [<__main__.DecisionTree object at 0x7fa0730da880>, <__main__.DecisionTree object at 0x7fa072f23670>]\n",
      "********* [<__main__.DecisionTree object at 0x7fa072f1aac0>, <__main__.DecisionTree object at 0x7fa0731285e0>]\n",
      "************ [<__main__.DecisionTree object at 0x7fa073128a60>, <__main__.DecisionTree object at 0x7fa07323cee0>]\n",
      "*************** [<__main__.DecisionTree object at 0x7fa07028b040>, <__main__.DecisionTree object at 0x7fa07028bac0>]\n",
      "****************** []\n",
      "****************** [<__main__.DecisionTree object at 0x7fa07028b580>, <__main__.DecisionTree object at 0x7fa07028bfd0>]\n",
      "********************* []\n",
      "********************* []\n",
      "*************** []\n",
      "************ [<__main__.DecisionTree object at 0x7fa07028bb50>, <__main__.DecisionTree object at 0x7fa07028b940>]\n",
      "*************** []\n",
      "*************** []\n",
      "********* []\n",
      "****** []\n"
     ]
    }
   ],
   "source": [
    "print_tree(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS METHOD PROVED TO BE SLOWER!\n",
    "\n",
    "import numpy as np \n",
    "import os\n",
    "import csv\n",
    "import timeit\n",
    "\n",
    "def load_data(data_path):\n",
    "    \"\"\"\n",
    "    Associated test case: tests/test_data.py\n",
    "\n",
    "    Reading and manipulating data is a vital skill for machine learning.\n",
    "\n",
    "    This function loads the data in data_path csv into two numpy arrays:\n",
    "    features (of size NxK) and targets (of size Nx1) where N is the number of rows\n",
    "    and K is the number of features. \n",
    "    \n",
    "    data_path leads to a csv comma-delimited file with each row corresponding to a \n",
    "    different example. Each row contains binary features for each example \n",
    "    (e.g. chocolate, fruity, caramel, etc.) The last column indicates the label for the\n",
    "    example how likely it is to win a head-to-head matchup with another candy \n",
    "    bar.\n",
    "\n",
    "    This function reads in the csv file, and reads each row into two numpy arrays.\n",
    "    The first array contains the features for each row. For example, in candy-data.csv\n",
    "    the features are:\n",
    "\n",
    "    chocolate,fruity,caramel,peanutyalmondy,nougat,crispedricewafer,hard,bar,pluribus\n",
    "\n",
    "    The second array contains the targets for each row. The targets are in the last \n",
    "    column of the csv file (labeled 'class'). The first row of the csv file contains \n",
    "    the labels for each column and shouldn't be read into an array.\n",
    "\n",
    "    Example:\n",
    "    chocolate,fruity,caramel,peanutyalmondy,nougat,crispedricewafer,hard,bar,pluribus,class\n",
    "    1,0,1,0,0,1,0,1,0,1\n",
    "\n",
    "    should be turned into:\n",
    "\n",
    "    [1,0,1,0,0,1,0,1,0] (features) and [1] (targets).\n",
    "\n",
    "    This should be done for each row in the csv file, making arrays of size NxK and Nx1.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): path to csv file containing the data\n",
    "\n",
    "    Output:\n",
    "        features (np.array): numpy array of size NxK containing the K features\n",
    "        targets (np.array): numpy array of size 1xN containing the N targets.\n",
    "        attribute_names (list): list of strings containing names of each attribute \n",
    "            (headers of csv)\n",
    "    \"\"\"\n",
    "\n",
    "    # Implementation 1 - taking one row at a time\n",
    "    # This implementation is useful if RAM size is a concern\n",
    "    with open(data_path, 'r', newline = '') as csvfile:\n",
    "        data = csv.reader(csvfile, delimiter = ',')\n",
    "        attribute_names = data.__next__()\n",
    "\n",
    "        col_len = len(attribute_names) - 1 # Number of features\n",
    "\n",
    "        features = np.array(data.__next__()[:col_len], dtype=int) # Initialize feature array\n",
    "    \n",
    "        for row in data:\n",
    "            features = np.vstack((features,row[:col_len]))\n",
    "        features = features.astype('int')\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, value=None, attribute_name=\"root\", attribute_index=None, branches=None):\n",
    "        \"\"\"\n",
    "        This class implements a tree structure with multiple branches at each node.\n",
    "        If self.branches is an empty list, this is a leaf node and what is contained in\n",
    "        self.value is the predicted class.\n",
    "\n",
    "        The defaults for this are for a root node in the tree.\n",
    "\n",
    "        Arguments:\n",
    "            branches (list): List of Tree classes. Used to traverse the tree. In a\n",
    "                binary decision tree, the length of this list is either 2 (for left and\n",
    "                right branches) or 0 (at a leaf node).\n",
    "            attribute_name (str): Contains name of attribute that the tree splits the data\n",
    "                on. Used for visualization (see `DecisionTree.visualize`).\n",
    "            attribute_index (float): Contains the  index of the feature vector for the\n",
    "                given attribute. Should match with self.attribute_name.\n",
    "            value (number): Contains the value that data should be compared to along the\n",
    "                given attribute.\n",
    "        \"\"\"\n",
    "        self.branches = [] if branches is None else branches\n",
    "        self.attribute_name = attribute_name\n",
    "        self.attribute_index = attribute_index\n",
    "        self.value = value\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, attribute_names):\n",
    "        \"\"\"\n",
    "        TODO: Implement this class.\n",
    "\n",
    "        This class implements a binary decision tree learner for examples with\n",
    "        categorical attributes. Use the ID3 algorithm for implementing the Decision\n",
    "        Tree: https://en.wikipedia.org/wiki/ID3_algorithm\n",
    "\n",
    "        A decision tree is a machine learning model that fits data with a tree\n",
    "        structure. Each branching point along the tree marks a decision (e.g.\n",
    "        today is sunny or today is not sunny). Data is filtered by the value of\n",
    "        each attribute to the next level of the tree. At the next level, the process\n",
    "        starts again with the remaining attributes, recursing on the filtered data.\n",
    "\n",
    "        Which attributes to split on at each point in the tree are decided by the\n",
    "        information gain of a specific attribute.\n",
    "\n",
    "        Here, you will implement a binary decision tree that uses the ID3 algorithm.\n",
    "        Your decision tree will be contained in `self.tree`, which consists of\n",
    "        nested Tree classes (see above).\n",
    "\n",
    "        Args:\n",
    "            attribute_names (list): list of strings containing the attribute names for\n",
    "                each feature (e.g. chocolatey, good_grades, etc.)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attribute_names = attribute_names\n",
    "        self.tree = None\n",
    "\n",
    "    def _check_input(self, features):\n",
    "        if features.shape[1] != len(self.attribute_names):\n",
    "            raise ValueError(\n",
    "                \"Number of features and number of attribute names must match!\"\n",
    "            )\n",
    "\n",
    "    def fit(self, features, targets):\n",
    "        \"\"\"\n",
    "        Takes in the features as a numpy array and fits a decision tree to the targets.\n",
    "\n",
    "        Args:\n",
    "            features (np.array): numpy array of size NxF containing features, where N is\n",
    "                number of examples and F is number of features.\n",
    "            targets (np.array): numpy array containing class labels for each of the N\n",
    "                examples.\n",
    "        Output:\n",
    "            VOID: It should update self.tree with a built decision tree.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"----- START ITERATION ------\")\n",
    "        \n",
    "        # Checking if features is now empty i.e. have we split by all possible features?\n",
    "        if len(self.attribute_names) <= 1:\n",
    "            # The predicted value should be the most common value in target\n",
    "            zeros = sum(targets == 0)\n",
    "            ones = sum(targets == 1)\n",
    "            pred_value = 1\n",
    "            if zeros >= ones:\n",
    "                pred_value = 0\n",
    "            print('--- ZERO/ONE FEATURE LEFT ---')\n",
    "            self.tree = Node(value = pred_value)\n",
    "            print('----- END RECURSION ----')\n",
    "            return self #Node(value = pred_value)\n",
    "        \n",
    "        # Need to handle this special case where I have just one feature left\n",
    "        # And it's getting cast as a 1-dim vector\n",
    "        # I need it to stay as a 2-dim vector with 1 column\n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(-1,1)\n",
    "        \n",
    "        print(\"FEATURES\")\n",
    "        print(features)\n",
    "        print(f\"TARGETS: {targets}\")\n",
    "        \n",
    "        print(f\"self.attribute_names = {self.attribute_names}\")\n",
    "\n",
    "        inf_gains = [information_gain(features, index, targets) for index in range(features.shape[1])]\n",
    "\n",
    "        max_gain = max(inf_gains)\n",
    "\n",
    "        # Finding the first feature with max gain \n",
    "        # There might be more, but we just take the first one since it doesn't really matter\n",
    "        for index in range(len(inf_gains)):\n",
    "            if inf_gains[index] == max_gain:\n",
    "                # the value of index is now the index where the max_gain is\n",
    "                break\n",
    "        print(f\"INDEX = {index}\")\n",
    "        # Recursion stopping criteria\n",
    "        # If the best feature we can split on doesn't have two values, then this is a leaf node\n",
    "        if len(set(features[:, index])) == 1:\n",
    "            # The predicted value should be the most common value in target\n",
    "            zeros = sum(targets == 0)\n",
    "            ones = sum(targets == 1)\n",
    "            pred_value = 1\n",
    "            if zeros >= ones:\n",
    "                pred_value = 0\n",
    "            print('--- BEST FEATURE HAS 1 VALUE ---')\n",
    "            print('----- END RECURSION ----')\n",
    "            return Node(value = pred_value)\n",
    "\n",
    "        self._check_input(features)\n",
    "        \n",
    "        remaining_attribute_names = [self.attribute_names[i] for i in range(len(self.attribute_names)) if i != index ]\n",
    "        remaining_attribute_indices = [ i for i in range(len(self.attribute_names)) if i != index]\n",
    "        print(f\"REMAINING ATTRIBUTE NAMES = {remaining_attribute_names}\")\n",
    "        print(f\"REMAINING ATTRIBUTE INDECES= {remaining_attribute_indices}\")\n",
    "        self.tree = Node(value = 0, attribute_name = self.attribute_names[index], attribute_index=index)\n",
    "\n",
    "        left_features = features[features[:,index] <= self.tree.value, :][:,remaining_attribute_indices]\n",
    "        left_targets = targets[features[:,index] <= self.tree.value]\n",
    "        \n",
    "#         print(left_features)\n",
    "        \n",
    "        right_features = features[features[:,index] > self.tree.value, :][:,remaining_attribute_indices]\n",
    "        right_targets = targets[features[:,index] > self.tree.value]\n",
    "        print(\"---- GOING DEEPER ON RECURSION ----\")\n",
    "        self.tree.branches.append(DecisionTree(remaining_attribute_names).fit(left_features, left_targets)) \n",
    "        self.tree.branches.append(DecisionTree(remaining_attribute_names).fit(right_features, right_targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_attributes = ['feature_1', 'feature_2', 'feature_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([[1,0,1], [0,1,0],[1,0,1],[1,1,1],[1,1,0],[1,1,0],[0,0,0],[0,1,0]])\n",
    "targets = np.array([1,1,1,1,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_DT = DecisionTree(example_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 1]\n",
      " [1 1 1]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [0 0 0]\n",
      " [0 1 0]] [1 1 1 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[features[:,2] <= 0, :][:, [0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- START ITERATION ------\n",
      "FEATURES\n",
      "[[1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 1]\n",
      " [1 1 1]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [0 0 0]\n",
      " [0 1 0]]\n",
      "TARGETS: [1 1 1 1 0 0 0 0]\n",
      "self.attribute_names = ['feature_1', 'feature_2', 'feature_3']\n",
      "INDEX = 2\n",
      "REMAINING ATTRIBUTE NAMES = ['feature_1', 'feature_2']\n",
      "REMAINING ATTRIBUTE INDECES= [0, 1]\n",
      "---- GOING DEEPER ON RECURSION ----\n",
      "----- START ITERATION ------\n",
      "FEATURES\n",
      "[[0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 1]]\n",
      "TARGETS: [1 0 0 0 0]\n",
      "self.attribute_names = ['feature_1', 'feature_2']\n",
      "INDEX = 0\n",
      "REMAINING ATTRIBUTE NAMES = ['feature_2']\n",
      "REMAINING ATTRIBUTE INDECES= [1]\n",
      "---- GOING DEEPER ON RECURSION ----\n",
      "----- START ITERATION ------\n",
      "--- ZERO/ONE FEATURE LEFT ---\n",
      "----- END RECURSION ----\n",
      "----- START ITERATION ------\n",
      "--- ZERO/ONE FEATURE LEFT ---\n",
      "----- END RECURSION ----\n",
      "----- START ITERATION ------\n",
      "FEATURES\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "TARGETS: [1 1 1]\n",
      "self.attribute_names = ['feature_1', 'feature_2']\n",
      "INDEX = 0\n",
      "--- BEST FEATURE HAS 1 VALUE ---\n",
      "----- END RECURSION ----\n"
     ]
    }
   ],
   "source": [
    "example_DT.fit(features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, <__main__.Node at 0x7fed6a9e6a00>]"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_DT.tree.branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left Branch\n",
    "example_DT.tree.branches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_DT.tree.branches[1].branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[features[:, 1] <= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-402-3396f65e96fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "np.array([1,2,3]).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.06 ms ± 32 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit load_data(data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw1-decision-trees",
   "language": "python",
   "name": "hw1-decision-trees"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
