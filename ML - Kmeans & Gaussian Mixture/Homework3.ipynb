{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def euclidean_distance(v, w):\n",
    "    \"\"\"Returns the Euclidean distance between two vectors\"\"\"\n",
    "\n",
    "    return math.sqrt(sum([(v[ii]-w[ii])**2 for ii in range(len(v))]))\n",
    "\n",
    "def euclidean_distances(X, Y):\n",
    "    \"\"\"Compute pairwise Euclidean distance between the rows of two matrices X (shape MxK) \n",
    "    and Y (shape NxK). The output of this function is a matrix of shape MxN containing\n",
    "    the Euclidean distance between two rows.\n",
    "    \n",
    "    Arguments:\n",
    "        X {np.ndarray} -- First matrix, containing M examples with K features each.\n",
    "        Y {np.ndarray} -- Second matrix, containing N examples with K features each.\n",
    "\n",
    "    Returns:\n",
    "        D {np.ndarray}: MxN matrix with Euclidean distances between rows of X and rows of Y.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = np.ndarray((X.shape[0], Y.shape[0]))\n",
    "    \n",
    "    for ii in range(X.shape[0]):\n",
    "        for kk in range(Y.shape[0]):\n",
    "            \n",
    "            result[ii,kk] = euclidean_distance(X[ii], Y[kk])\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def update_assignments(features, means):\n",
    "    \"\"\"\n",
    "    Takes features & means and returns label assignments \n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): data that needs labels\n",
    "        means (np.ndarray): # of rows equal to number of means; \n",
    "                            # of columns equal to # of columns of feature\n",
    "                            i.e. dimensionality is the same\n",
    "\n",
    "    Returns:\n",
    "        labels (np.ndarray): array of labels based on which cluster mean is closes\n",
    "            Has length equal to number of rows in features i.e. one label per feature\n",
    "    \"\"\"\n",
    "\n",
    "    # First, calculate distances between features and means\n",
    "    # A row are all the distances between a given feature and all the means\n",
    "    # A column are all the distances between a mean and all the features\n",
    "    distances = euclidean_distances(features, means)\n",
    "    # the dimensions of distances are:\n",
    "    # rows = number of rows in features\n",
    "    # columns = number of means i.e. number of clusters we are looking for\n",
    "\n",
    "    # Then, we label each feature based on the mean that's closest to it\n",
    "    # The below gives the index at each row where the minimum value is found in that row\n",
    "    feature_labels = np.argmin(distances, axis=1)\n",
    "\n",
    "    return feature_labels\n",
    "\n",
    "def update_means(features, feature_label, n_means):\n",
    "    \"\"\"\n",
    "    Takes features & their labels and returns the means for each label class\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): dataset to determine new means\n",
    "        feature_labels (np.ndarray): 1D array with the labels for the features\n",
    "        n_means (int): number of means to find. Should be equal to n_clusters\n",
    "\n",
    "    Returns:\n",
    "        new_means (np.ndarray): \n",
    "            # of rows = number of means\n",
    "            # of columns = dimensionality of features\n",
    "    \"\"\"\n",
    "\n",
    "    new_means = []\n",
    "\n",
    "    for label in range(n_means):\n",
    "\n",
    "        new_means.append(np.mean(features[feature_labels==label], axis=0))\n",
    "    \n",
    "    return np.array(new_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "def generate_cluster_data(\n",
    "    n_samples=20,\n",
    "    n_features=3,\n",
    "    n_centers=3,\n",
    "    cluster_stds=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate numpy arrays that are clusterable into `n_centers` clusters using your\n",
    "    implementations of KMeans, Soft KMeans, and Gaussian Mixture Model. This function\n",
    "    uses make_blobs and is implemented for you.\n",
    "\n",
    "    UPDATE: THIS FUNCTION IS IMPLEMENTED FOR YOU.\n",
    "\n",
    "    The generated data should be a set of Gaussian blobs in n_features-dimensional\n",
    "    space. The means (e.g. locations) of these blobs are generated randomly by you.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): Number of samples to generate\n",
    "        n_features (int): Number of features for each sample\n",
    "        n_centers (int): Number of clusters to generate\n",
    "        cluster_stds (float or sequence of floats): standard deviation for each cluster.\n",
    "            If a single float, then each cluster has the same standard deviation. If a\n",
    "            sequence, the length of the sequence should match n_centers, and each cluster\n",
    "            will have that as the standard deviation.\n",
    "    Returns:\n",
    "        X (np.ndarray of shape (n_samples, n_features): A numpy array containing the\n",
    "            generated data. Each row represents a point in n_features-dimensional space.\n",
    "            X should be clusterable into n_centers number of clusters.\n",
    "        y (np.ndarray of shape (n_samples,): A numpy array containing the cluster labels\n",
    "            for the generated data. Each element tells you which cluster each data point\n",
    "            came from. The actual labels can be arbitrary but points belonging to\n",
    "            different clusters should have different labels. Labels should be 0 indexed,\n",
    "            with labels ranging from 0,...,(n_centers-1).\n",
    "\n",
    "    \"\"\"\n",
    "    return make_blobs(\n",
    "        n_samples=n_samples, n_features=n_features, centers=n_centers, cluster_std=cluster_stds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, real_labels = generate_cluster_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.04304139,  5.5021832 ,  3.95974769],\n",
       "       [ 4.85495409,  6.69190869, -6.05763649],\n",
       "       [ 5.87925234,  7.95374633, -0.85284066],\n",
       "       [ 4.02985394,  6.36097497, -6.66161406],\n",
       "       [ 5.19526046,  9.39846708, -1.56651801],\n",
       "       [ 5.48615942,  6.49121122, -7.07251071],\n",
       "       [ 5.93517564,  5.8240991 , -2.3030178 ],\n",
       "       [-7.10911273,  4.12536751,  4.87128736],\n",
       "       [ 6.34822237, 10.17951786, -0.0315085 ],\n",
       "       [-7.33237383,  5.39148511,  4.68269291],\n",
       "       [ 5.28297296,  9.55330808, -0.24186208],\n",
       "       [ 3.72684682,  6.7018912 , -4.8968114 ],\n",
       "       [ 5.80032905,  8.85764183, -7.03130183],\n",
       "       [ 3.97323494,  7.42896226, -6.37462941],\n",
       "       [ 5.08852401,  7.56940568, -1.07259466],\n",
       "       [-7.50899686,  3.76588995,  1.1079059 ],\n",
       "       [-6.1412839 ,  4.26948748,  1.93578844],\n",
       "       [ 3.94621969,  7.25422937, -7.11175872],\n",
       "       [ 6.08318513,  8.80597924, -0.54329551],\n",
       "       [-6.1088858 ,  4.51101209,  4.19360737]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 1, 0, 1, 0, 2, 0, 2, 0, 1, 1, 1, 0, 2, 2, 1, 0, 2])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.50899686,  3.76588995, -7.11175872])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mins = np.amin(features, axis=0)\n",
    "mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.34822237, 10.17951786,  4.87128736])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxs = np.amax(features, axis=0)\n",
    "maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.04469205,  5.36929693, -4.1159972 ],\n",
       "       [-0.58038725,  6.97270391, -1.12023568],\n",
       "       [ 2.88391756,  8.57611088,  1.87552584]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the cluster means\n",
    "n_clusters = 3\n",
    "means = np.linspace(mins, maxs, n_clusters+1, endpoint=False)[1:,]\n",
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize labels\n",
    "labels = np.zeros(features.shape[0])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION\n",
      "[1 1 2 1 2 1 2 1 2 1 2 1 1 1 2 0 0 1 2 1]\n",
      "ITERATION\n",
      "[2 1 0 1 0 1 1 2 0 2 0 1 1 1 0 2 2 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "while sum(labels != update_assignments(features, means)) > 0:\n",
    "    print(\"ITERATION\")\n",
    "    labels = update_assignments(features, means)\n",
    "    print(labels)\n",
    "    means = update_means(features, labels, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.64534074,  7.08874048, -0.93972416],\n",
       "       [ 2.88735941,  6.57384281, -4.22193101],\n",
       "       [ 0.36356496,  6.79607544,  0.12039786]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13.39672689,  8.65950432,  5.30452653],\n",
       "       [ 6.81678258,  9.42544364, 13.7421146 ],\n",
       "       [ 6.06447972,  8.38876134, 12.71223751],\n",
       "       [ 7.9433111 ,  5.20791974,  6.98462081],\n",
       "       [ 6.67102076,  9.67464413, 14.15384128],\n",
       "       [12.13389403,  7.51853187,  4.8437095 ],\n",
       "       [ 8.33913968,  5.33929597,  6.71648533],\n",
       "       [ 4.84223992,  7.35421497, 11.9313843 ],\n",
       "       [ 9.06934886,  4.79985791,  4.63254209],\n",
       "       [ 7.77197164,  9.89584205, 13.89561439],\n",
       "       [14.30026985,  9.41352674,  5.51067297],\n",
       "       [ 8.17399754,  5.70240449,  7.47374479],\n",
       "       [ 8.9293525 ,  5.66298113,  6.48395231],\n",
       "       [ 8.18367444,  5.33165663,  6.89324746],\n",
       "       [12.02224371,  7.65616948,  5.50779277],\n",
       "       [10.95569136,  6.81078951,  5.51205925],\n",
       "       [ 8.33717577,  4.5076067 ,  5.36328254],\n",
       "       [ 8.07959903,  9.90166916, 13.72743409],\n",
       "       [ 7.2405839 ,  8.75071563, 12.58412714],\n",
       "       [13.04865008,  8.28768436,  4.97397407]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are distances between features and means\n",
    "# A row are all the distances between a given feature and all the means\n",
    "# A column are all the distances between a mean and all the features\n",
    "distances = euclidean_distances(features, means)\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 1, 0, 2, 1, 0, 2, 0, 2, 1, 1, 1, 2, 2, 1, 0, 0, 2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labelling each feature based on its closes mean\n",
    "feature_labels = np.argmin(distances, axis=1)\n",
    "feature_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_means = []\n",
    "for mean in range(n_clusters):\n",
    "    new_means.append(np.mean(features[feature_labels==mean], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.94858756, -9.59070442,  6.34736238],\n",
       "       [ 1.67771032,  1.28316584, 10.22437548],\n",
       "       [ 4.07312124,  5.66769647,  9.29381835]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(new_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.72124642,  -9.93750298,   5.51181559],\n",
       "       [  8.47841163,  -9.15706657,   6.4760494 ],\n",
       "       [  8.08241219, -10.6963171 ,   6.6713693 ],\n",
       "       [  7.32403053,  -8.60731134,   7.65594465],\n",
       "       [  9.76545682, -10.34458622,   6.80561336],\n",
       "       [ 10.34848236,  -9.8336546 ,   5.85196568],\n",
       "       [  9.920073  ,  -8.55849214,   5.45877872]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[feature_labels==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.94858756, -9.59070442,  6.34736238])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mean = np.mean(features[feature_labels==0], axis=0)\n",
    "new_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.67771032,  1.28316584, 10.22437548])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(features[feature_labels==1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.07312124, 5.66769647, 9.29381835])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(features[feature_labels==2], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Full Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def euclidean_distance(v, w):\n",
    "    \"\"\"Returns the Euclidean distance between two vectors\"\"\"\n",
    "\n",
    "    return math.sqrt(sum([(v[ii]-w[ii])**2 for ii in range(len(v))]))\n",
    "\n",
    "def euclidean_distances(X, Y):\n",
    "    \"\"\"Compute pairwise Euclidean distance between the rows of two matrices X (shape MxK) \n",
    "    and Y (shape NxK). The output of this function is a matrix of shape MxN containing\n",
    "    the Euclidean distance between two rows.\n",
    "    \n",
    "    Arguments:\n",
    "        X {np.ndarray} -- First matrix, containing M examples with K features each.\n",
    "        Y {np.ndarray} -- Second matrix, containing N examples with K features each.\n",
    "\n",
    "    Returns:\n",
    "        D {np.ndarray}: MxN matrix with Euclidean distances between rows of X and rows of Y.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = np.ndarray((X.shape[0], Y.shape[0]))\n",
    "    \n",
    "    for ii in range(X.shape[0]):\n",
    "        for kk in range(Y.shape[0]):\n",
    "            \n",
    "            result[ii,kk] = euclidean_distance(X[ii], Y[kk])\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def update_assignments(features, means):\n",
    "    \"\"\"\n",
    "    Takes features & means and returns label assignments \n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): data that needs labels\n",
    "        means (np.ndarray): # of rows equal to number of means; \n",
    "                            # of columns equal to # of columns of feature\n",
    "                            i.e. dimensionality is the same\n",
    "\n",
    "    Returns:\n",
    "        labels (np.ndarray): array of labels based on which cluster mean is closes\n",
    "            Has length equal to number of rows in features i.e. one label per feature\n",
    "    \"\"\"\n",
    "\n",
    "    # First, calculate distances between features and means\n",
    "    # A row are all the distances between a given feature and all the means\n",
    "    # A column are all the distances between a mean and all the features\n",
    "    distances = euclidean_distances(features, means)\n",
    "    # the dimensions of distances are:\n",
    "    # rows = number of rows in features\n",
    "    # columns = number of means i.e. number of clusters we are looking for\n",
    "\n",
    "    # Then, we label each feature based on the mean that's closest to it\n",
    "    # The below gives the index at each row where the minimum value is found in that row\n",
    "    feature_labels = np.argmin(distances, axis=1)\n",
    "\n",
    "    return feature_labels\n",
    "\n",
    "def update_means(features, feature_labels, n_means):\n",
    "    \"\"\"\n",
    "    Takes features & their labels and returns the means for each label class\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): dataset to determine new means\n",
    "        feature_labels (np.ndarray): 1D array with the labels for the features\n",
    "        n_means (int): number of means to find. Should be equal to n_clusters\n",
    "\n",
    "    Returns:\n",
    "        new_means (np.ndarray): \n",
    "            # of rows = number of means\n",
    "            # of columns = dimensionality of features\n",
    "    \"\"\"\n",
    "\n",
    "    new_means = []\n",
    "\n",
    "    for label in range(n_means):\n",
    "\n",
    "        new_means.append(np.mean(features[feature_labels==label], axis=0))\n",
    "    \n",
    "    return np.array(new_means)\n",
    "\n",
    "\n",
    "class KMeans():\n",
    "    def __init__(self, n_clusters):\n",
    "        \"\"\"\n",
    "        This class implements the traditional KMeans algorithm with hard assignments:\n",
    "\n",
    "        https://en.wikipedia.org/wiki/K-means_clustering\n",
    "\n",
    "        The KMeans algorithm has two steps:\n",
    "\n",
    "        1. Update assignments\n",
    "        2. Update the means\n",
    "\n",
    "        While you only have to implement the fit and predict functions to pass the\n",
    "        test cases, we recommend that you use an update_assignments function and an\n",
    "        update_means function internally for the class.\n",
    "\n",
    "        Use only numpy to implement this algorithm.\n",
    "\n",
    "        Args:\n",
    "            n_clusters (int): Number of clusters to cluster the given data into.\n",
    "\n",
    "        \"\"\"\n",
    "        self.n_clusters = n_clusters\n",
    "        self.means = None\n",
    "\n",
    "    def fit(self, features):\n",
    "        \"\"\"\n",
    "        Fit KMeans to the given data using `self.n_clusters` number of clusters.\n",
    "        Features can have greater than 2 dimensions.\n",
    "\n",
    "        Args:\n",
    "            features (np.ndarray): array containing inputs of size\n",
    "                (n_samples, n_features).\n",
    "        Returns:\n",
    "            None (saves model - means - internally)\n",
    "        \"\"\"\n",
    "        \n",
    "        # INITIALIZING MEANS:\n",
    "\n",
    "        # First, I will find the min and max values along each dimension\n",
    "        mins = np.amin(features, axis=0)\n",
    "        maxs = np.amax(features, axis=0)\n",
    "\n",
    "        # Then I will initialize the means as equally distances along all dimensions\n",
    "        # We are ignoring the 0-th point, because that is at the very minimum\n",
    "        # We are generating n_clusters+1 points, because we are ignoring 1 point\n",
    "        self.means = np.linspace(mins, maxs, self.n_clusters+1, endpoint=False)[1:,]\n",
    "\n",
    "        # Initialize labels\n",
    "        labels = np.zeros(features.shape[0])\n",
    "\n",
    "        while sum(labels != update_assignments(features, self.means)) > 0:\n",
    "            labels = update_assignments(features, self.means)\n",
    "            self.means = update_means(features, labels, self.n_clusters)\n",
    "\n",
    "\n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Given features, an np.ndarray of size (n_samples, n_features), predict cluster\n",
    "        membership labels.\n",
    "\n",
    "        Args:\n",
    "            features (np.ndarray): array containing inputs of size\n",
    "                (n_samples, n_features).\n",
    "        Returns:\n",
    "            predictions (np.ndarray): predicted cluster membership for each features,\n",
    "                of size (n_samples,). Each element of the array is the index of the\n",
    "                cluster the sample belongs to.\n",
    "        \"\"\"\n",
    "\n",
    "        return update_assignments(features, self.means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adjusted_mutual_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-193-a3cd9957c73c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;31m# predict and calculate adjusted mutual info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjusted_mutual_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adjusted_mutual_info' is not defined"
     ]
    }
   ],
   "source": [
    "n_samples = [1000, 10000]\n",
    "n_centers = [2]\n",
    "stds = [.1]\n",
    "n_features = [1, 2, 4]\n",
    "\n",
    "for n in n_samples:\n",
    "    for f in n_features:\n",
    "        for c in n_centers:\n",
    "            for s in stds:\n",
    "                features, targets = generate_cluster_data(\n",
    "                    n_samples=n,\n",
    "                    n_features=f,\n",
    "                    n_centers=c,\n",
    "                    cluster_stds=s\n",
    "                )\n",
    "                # make model and fit\n",
    "                model = KMeans(c)\n",
    "                model.fit(features)\n",
    "\n",
    "                means = model.means\n",
    "                orderings = permutations(means)\n",
    "                distance_to_true_means = []\n",
    "\n",
    "                actual_means = np.array([\n",
    "                    features[targets == i, :].mean(axis=0) for i in range(targets.max() + 1)\n",
    "                ])\n",
    "\n",
    "                for ordering in orderings:\n",
    "                    _means = np.array(list(ordering))\n",
    "\n",
    "                    distance_to_true_means.append(\n",
    "                        np.abs(_means - actual_means).sum()\n",
    "                    )\n",
    "\n",
    "                assert (min(distance_to_true_means) < 1e-1)\n",
    "\n",
    "                # predict and calculate adjusted mutual info\n",
    "                labels = model.predict(features)\n",
    "                acc = adjusted_mutual_info(targets, labels)\n",
    "                assert (acc >= .9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets = generate_cluster_data(n_samples=100, n_features=2, cluster_stds=.1, n_centers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35738862, 0.31005691, 0.8978578 ],\n",
       "       [0.69377601, 0.09184041, 0.83560442]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.40942296e+00,  8.66170967e+00,  1.91751287e-01],\n",
       "       [-4.47451404e+00,  8.54177088e+00,  2.38118123e-01],\n",
       "       [-8.65337680e+00,  7.84925275e+00, -9.54449812e+00],\n",
       "       [-4.31214719e+00,  8.81776864e+00,  6.36673178e-02],\n",
       "       [-8.51216783e+00,  7.91912032e+00, -9.40504443e+00],\n",
       "       [-8.59701884e+00,  7.74940894e+00, -9.44735198e+00],\n",
       "       [-4.45491879e+00,  8.71350229e+00,  1.88522438e-01],\n",
       "       [-8.69564705e+00,  7.72639158e+00, -9.34008574e+00],\n",
       "       [-4.40561814e+00,  8.77894958e+00,  2.14360522e-01],\n",
       "       [-8.51993422e+00,  7.77162973e+00, -9.37052407e+00],\n",
       "       [-8.76916621e+00,  7.82362513e+00, -9.38206544e+00],\n",
       "       [-8.66790581e+00,  7.91166674e+00, -9.35972850e+00],\n",
       "       [-8.52595701e+00,  7.96343433e+00, -9.49986339e+00],\n",
       "       [-8.48246257e+00,  7.84386565e+00, -9.45929351e+00],\n",
       "       [-4.42964670e+00,  8.88292148e+00,  2.23248027e-01],\n",
       "       [-8.57870427e+00,  7.82675145e+00, -9.42061631e+00],\n",
       "       [-8.65665138e+00,  7.78002252e+00, -9.51351615e+00],\n",
       "       [-8.47181687e+00,  7.88408068e+00, -9.30813309e+00],\n",
       "       [-4.51852406e+00,  8.80209943e+00,  1.58309197e-01],\n",
       "       [-4.38630229e+00,  8.86626945e+00,  1.48039901e-01],\n",
       "       [-4.21142251e+00,  8.54185343e+00,  1.79942699e-01],\n",
       "       [-8.60928777e+00,  8.06657553e+00, -9.44408960e+00],\n",
       "       [-4.52780287e+00,  8.82358963e+00,  3.82374118e-01],\n",
       "       [-8.69572420e+00,  7.77035122e+00, -9.44150728e+00],\n",
       "       [-8.62309466e+00,  7.84888315e+00, -9.47021470e+00],\n",
       "       [-4.29645250e+00,  8.87121486e+00,  9.70153103e-02],\n",
       "       [-8.58439488e+00,  7.86232262e+00, -9.41785460e+00],\n",
       "       [-8.72861229e+00,  7.70965711e+00, -9.52642840e+00],\n",
       "       [-4.22572418e+00,  8.57422967e+00, -7.25497299e-03],\n",
       "       [-4.40781352e+00,  8.60220707e+00,  1.92915519e-01],\n",
       "       [-4.50534080e+00,  8.67155504e+00,  2.58075977e-01],\n",
       "       [-8.62945454e+00,  7.90353930e+00, -9.37056307e+00],\n",
       "       [-8.53612415e+00,  7.79368150e+00, -9.20931041e+00],\n",
       "       [-4.52074941e+00,  8.86578245e+00,  1.34895768e-01],\n",
       "       [-4.60082521e+00,  8.48173955e+00,  1.24028116e-01],\n",
       "       [-4.39067313e+00,  8.72499462e+00,  2.39090719e-01],\n",
       "       [-4.32508031e+00,  8.71587429e+00,  2.37222349e-01],\n",
       "       [-4.36711497e+00,  8.95653290e+00,  1.69780583e-01],\n",
       "       [-8.71720205e+00,  7.76563158e+00, -9.38388434e+00],\n",
       "       [-4.41689209e+00,  8.68519952e+00,  1.71548774e-01],\n",
       "       [-4.39757277e+00,  8.74962471e+00,  2.13224499e-01],\n",
       "       [-4.30009241e+00,  8.86629216e+00,  1.24983850e-01],\n",
       "       [-8.70140725e+00,  7.65680923e+00, -9.49045591e+00],\n",
       "       [-8.65245487e+00,  7.78571122e+00, -9.40332117e+00],\n",
       "       [-4.55393615e+00,  8.59019866e+00,  1.24787085e-01],\n",
       "       [-8.68329808e+00,  7.89742696e+00, -9.52527048e+00],\n",
       "       [-8.64338457e+00,  7.86094710e+00, -9.48448159e+00],\n",
       "       [-4.55654459e+00,  8.80716623e+00,  1.51407918e-01],\n",
       "       [-8.62025116e+00,  7.64888761e+00, -9.45999476e+00],\n",
       "       [-4.48698458e+00,  8.73984607e+00,  3.71878601e-01],\n",
       "       [-8.52770872e+00,  7.87059134e+00, -9.56861452e+00],\n",
       "       [-8.51890715e+00,  7.94832657e+00, -9.36517305e+00],\n",
       "       [-4.63272058e+00,  8.80290314e+00,  2.72191965e-01],\n",
       "       [-8.60121990e+00,  7.88743787e+00, -9.51116532e+00],\n",
       "       [-4.40708855e+00,  8.86952951e+00,  1.28056223e-01],\n",
       "       [-8.57980922e+00,  7.65893012e+00, -9.37886569e+00],\n",
       "       [-4.54080877e+00,  8.74965685e+00,  1.15509877e-02],\n",
       "       [-4.50411981e+00,  8.79958345e+00,  3.73775129e-01],\n",
       "       [-4.45176711e+00,  8.79933211e+00,  5.14908389e-02],\n",
       "       [-8.63269257e+00,  7.77998600e+00, -9.35288153e+00],\n",
       "       [-4.32438329e+00,  8.71744343e+00,  5.48064336e-02],\n",
       "       [-4.38783022e+00,  8.57344095e+00,  2.75242511e-01],\n",
       "       [-8.42228561e+00,  8.06319249e+00, -9.35656649e+00],\n",
       "       [-8.60738401e+00,  7.76810007e+00, -9.48032910e+00],\n",
       "       [-8.80925361e+00,  7.94481867e+00, -9.33139961e+00],\n",
       "       [-4.36991398e+00,  8.82189088e+00,  2.32628310e-01],\n",
       "       [-4.44338123e+00,  8.68795385e+00,  4.04636659e-02],\n",
       "       [-4.26075643e+00,  8.79505563e+00,  2.20306909e-01],\n",
       "       [-4.48860228e+00,  8.54074660e+00,  1.74852452e-01],\n",
       "       [-4.42678109e+00,  8.59803064e+00,  1.17937340e-01],\n",
       "       [-4.45505048e+00,  8.83247109e+00,  1.85367229e-01],\n",
       "       [-4.38394098e+00,  8.86401729e+00, -3.39225971e-02],\n",
       "       [-8.54400349e+00,  7.89299249e+00, -9.40853059e+00],\n",
       "       [-8.52535113e+00,  7.80783081e+00, -9.50987337e+00],\n",
       "       [-4.31902787e+00,  8.74630717e+00,  2.43346064e-01],\n",
       "       [-8.60149109e+00,  7.92462749e+00, -9.56386671e+00],\n",
       "       [-8.64551182e+00,  7.76519456e+00, -9.45097768e+00],\n",
       "       [-8.68432475e+00,  7.82289123e+00, -9.67004916e+00],\n",
       "       [-8.61442528e+00,  7.94889310e+00, -9.31099515e+00],\n",
       "       [-8.61073913e+00,  7.98003485e+00, -9.25025840e+00],\n",
       "       [-4.25521835e+00,  8.80688303e+00,  1.31234068e-01],\n",
       "       [-8.72480179e+00,  7.77802645e+00, -9.49238271e+00],\n",
       "       [-4.40609420e+00,  8.68935313e+00,  2.82079022e-01],\n",
       "       [-4.35440541e+00,  8.69297446e+00,  1.93015061e-01],\n",
       "       [-4.55685007e+00,  8.80489532e+00,  1.75858501e-01],\n",
       "       [-4.46208669e+00,  8.78004621e+00,  1.22297469e-01],\n",
       "       [-4.35574329e+00,  8.85828821e+00,  1.55880702e-01],\n",
       "       [-8.47778438e+00,  7.79722565e+00, -9.39531947e+00],\n",
       "       [-8.57100486e+00,  7.56658937e+00, -9.43572059e+00],\n",
       "       [-8.59053827e+00,  7.87347237e+00, -9.47351412e+00],\n",
       "       [-8.75927101e+00,  7.82758120e+00, -9.39338565e+00],\n",
       "       [-4.46815201e+00,  8.75667398e+00,  2.26025138e-01],\n",
       "       [-4.31129374e+00,  8.70790917e+00,  7.37057172e-02],\n",
       "       [-8.51326117e+00,  7.92570306e+00, -9.47192013e+00],\n",
       "       [-4.43067434e+00,  8.75171284e+00,  1.69637870e-01],\n",
       "       [-8.76518489e+00,  7.71094734e+00, -9.42894277e+00],\n",
       "       [-8.43611979e+00,  7.78015861e+00, -9.43198075e+00],\n",
       "       [-8.72103668e+00,  7.85522629e+00, -9.52889999e+00],\n",
       "       [-8.39267173e+00,  7.84918113e+00, -9.33596919e+00],\n",
       "       [-4.41924109e+00,  8.63506455e+00,  1.62786152e-01]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import rand\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x=features[:,0], y=features[:,1], c=targets) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    " # make model and fit\n",
    "model = KMeans(2)\n",
    "model.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.27536505],\n",
       "       [-5.82459676]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-228-8281727c9a18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'log' is not defined"
     ]
    }
   ],
   "source": [
    "multivariate_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-20.87813454, -21.42914933, -22.0462887 , -38.01463882,\n",
       "       -20.16723205, -37.84066674, -37.8666099 , -39.40813209,\n",
       "       -20.881594  , -37.80558984, -20.68157094, -19.73828819,\n",
       "       -37.55850201, -20.04347252, -37.28612852, -20.97909451,\n",
       "       -20.738748  , -37.09015696, -20.91329002, -37.64148419,\n",
       "       -20.61039705, -21.76421353, -38.12123765, -38.13221327,\n",
       "       -37.20509886, -37.60540728, -20.86984158, -20.2130931 ,\n",
       "       -37.3914962 , -21.06865806, -20.33994155, -38.99333904,\n",
       "       -37.07094772, -38.07697168, -21.1499733 , -20.17678684,\n",
       "       -20.10860106, -39.06555479, -21.2153524 , -37.52193448,\n",
       "       -38.69740178, -38.9111942 , -18.94243351, -19.86251698,\n",
       "       -20.59344887, -21.4714526 , -37.66604789, -20.7046287 ,\n",
       "       -36.65403645, -21.2251545 , -38.53754607, -38.33272679,\n",
       "       -37.07992233, -20.64245955, -38.36201753, -20.57826068,\n",
       "       -39.13564574, -37.56456672, -21.66999266, -36.02049506,\n",
       "       -38.6956594 , -37.26506305, -21.00663148, -38.69889658,\n",
       "       -38.61864231, -37.98871966, -20.83772207, -37.75945578,\n",
       "       -21.42205626, -38.69702098, -20.65799805, -36.71374584,\n",
       "       -38.86721718, -37.18109603, -21.20355927, -37.48551102,\n",
       "       -37.82119086, -20.70325159, -20.31184522, -38.10364266,\n",
       "       -37.1643258 , -22.33092863, -37.39257754, -20.9845279 ,\n",
       "       -19.79580103, -20.50127546, -38.80183873, -20.25047427,\n",
       "       -20.44111126, -19.67291608, -37.48920637, -38.77678367,\n",
       "       -39.03536466, -38.47617271, -19.93537143, -20.28890302,\n",
       "       -19.91705288, -20.94437321, -20.18290462, -20.56403897])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multivariate_normal(mean=np.array([0,0]), cov=np.array([[1,0],[0,1]])).logpdf(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def convert_to_diag(x):\n",
    "    \"\"\"Helped function to set all non-diagonal values in a matrix to 0.\"\"\"\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if i != j:\n",
    "                x[i,j]=0\n",
    "                \n",
    "    return x\n",
    "\n",
    "class GMM():\n",
    "    def __init__(self, n_clusters, covariance_type):\n",
    "        \"\"\"\n",
    "        This class implements a Gaussian Mixture Model updated using expectation\n",
    "        maximization.\n",
    "\n",
    "        A useful tutorial:\n",
    "            https://campuspro-uploads.s3.us-west-2.amazonaws.com/63aa7cea-5e9c-4b62-96b7-8bbf3bc31b76/3a1d9101-8748-4e85-9830-4e45ffe1ca8d/EM%20derivations.pdf\n",
    "\n",
    "        The EM algorithm for GMMs has two steps:\n",
    "\n",
    "        1. Update posteriors (assignments to each Gaussian)\n",
    "        2. Update Gaussian parameters (means, variances, and priors for each Gaussian)\n",
    "\n",
    "        While you only have to implement the fit and predict functions to pass the\n",
    "        test cases, we recommend that you break these two steps apart into separate\n",
    "        functions. We have provided a template for you to put your code in.\n",
    "\n",
    "        Use only numpy to implement this algorithm.\n",
    "\n",
    "        This function MUST, after running 'fit', have variables named 'means' and\n",
    "        'covariances' in order to pass the test cases. These variables are checked by the\n",
    "        test cases to make sure you have recovered cluster parameters accurately.\n",
    "\n",
    "        The fit and predict functions are implemented for you. To complete the implementation,\n",
    "        you must implement:\n",
    "            - _e_step\n",
    "            - _m_step\n",
    "            - _log_likelihood\n",
    "\n",
    "        Args:\n",
    "            n_clusters (int): Number of Gaussians to cluster the given data into.\n",
    "            covariance_type (str): Either 'spherical', 'diagonal'. Determines the\n",
    "                covariance type for the Gaussians in the mixture model.\n",
    "\n",
    "        \"\"\"\n",
    "        self.n_clusters = n_clusters\n",
    "        allowed_covariance_types = ['spherical', 'diagonal']\n",
    "        if covariance_type not in allowed_covariance_types:\n",
    "            raise ValueError(f'covariance_type must be in {allowed_covariance_types}')\n",
    "        self.covariance_type = covariance_type\n",
    "\n",
    "        self.means = None\n",
    "        self.covariances = None\n",
    "        self.mixing_weights = None\n",
    "        self.max_iterations = 200\n",
    "\n",
    "    def fit(self, features):\n",
    "        \"\"\"\n",
    "        Fit GMM to the given data using `self.n_clusters` number of Gaussians.\n",
    "        Features can have greater than 2 dimensions.\n",
    "\n",
    "        Args:\n",
    "            features (np.ndarray): array containing inputs of size\n",
    "                (n_samples, n_features).\n",
    "        Returns:\n",
    "            None (saves model - means, covariances, and mixing weights - internally)\n",
    "        \"\"\"\n",
    "        # 1. Use your KMeans implementation to initialize the means of the GMM.\n",
    "        kmeans = KMeans(self.n_clusters)\n",
    "        kmeans.fit(features)\n",
    "        self.means = kmeans.means\n",
    "\n",
    "        # 2. Initialize the covariance matrix and the mixing weights\n",
    "        self.covariances = self._init_covariance(features.shape[-1])\n",
    "\n",
    "        # 3. Initialize the mixing weights\n",
    "        self.mixing_weights = np.random.rand(self.n_clusters)\n",
    "        self.mixing_weights /= np.sum(self.mixing_weights)\n",
    "\n",
    "        # 4. Compute log_likelihood under initial random covariance and KMeans means.\n",
    "        prev_log_likelihood = -float('inf')\n",
    "        log_likelihood = self._overall_log_likelihood(features)\n",
    "\n",
    "        # 5. While the log_likelihood is increasing significantly, or max_iterations has\n",
    "        # not been reached, continue EM until convergence.\n",
    "        n_iter = 0\n",
    "        while log_likelihood - prev_log_likelihood > 1e-4 and n_iter < self.max_iterations:\n",
    "            prev_log_likelihood = log_likelihood\n",
    "\n",
    "            assignments = self._e_step(features)\n",
    "            self.means, self.covariances, self.mixing_weights = (\n",
    "                self._m_step(features, assignments)\n",
    "            )\n",
    "\n",
    "            log_likelihood = self._overall_log_likelihood(features)\n",
    "            n_iter += 1\n",
    "\n",
    "        # Since self.covariances is assumed to have a certain shape, I need to reshape it in the end\n",
    "        if self.covariance_type == 'spherical':\n",
    "            # self.covariances is assumed to be a 1-D array of variances\n",
    "            final_covariances = np.zeros(self.n_clusters)\n",
    "\n",
    "            for index, covariance in enumerate(self.covariances):\n",
    "                final_covariances[index] = np.mean(np.diagonal(covariance))\n",
    "\n",
    "            self.covariances = final_covariances\n",
    "\n",
    "        elif self.covariance_type == 'diagonal':\n",
    "            # self.covariances is assumed to be a 2-D array \n",
    "            final_covariances = np.zeros((self.n_clusters, features.shape[-1]))\n",
    "\n",
    "            for index, covariance in enumerate(self.covariances):\n",
    "                final_covariances[index] = np.diagonal(covariance)\n",
    "\n",
    "            self.covariances = final_covariances\n",
    "\n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Given features, an np.ndarray of size (n_samples, n_features), predict the label\n",
    "        of each sample (e.g. the index of the Gaussian with the highest posterior for that\n",
    "        sample).\n",
    "\n",
    "        Args:\n",
    "            features (np.ndarray): array containing inputs of size\n",
    "                (n_samples, n_features).\n",
    "        Returns:\n",
    "            predictions (np.ndarray): predicted assigment to each cluster for each sample,\n",
    "                of size (n_samples,). Each element is which cluster that sample belongs to.\n",
    "        \"\"\"\n",
    "        posteriors = self._e_step(features)\n",
    "        return np.argmax(posteriors, axis=-1)\n",
    "\n",
    "    def _e_step(self, features):\n",
    "        \"\"\"\n",
    "        The expectation step in Expectation-Maximization. Given the current class member\n",
    "        variables self.mean, self.covariance, and self.mixing_weights:\n",
    "            1. Calculate the log_likelihood of each point under each Gaussian.\n",
    "            2. Calculate the posterior probability for each point under each Gaussian\n",
    "            3. Return the posterior probability (assignments).\n",
    "        \n",
    "        This function should call your implementation of of _posterior which in turn calls \n",
    "        _log_likelihood (which should call\n",
    "        multivariate_normal.logpdf). This should use the Gaussian parameter contained in\n",
    "        self.means, self.covariance, and self.mixing_weights\n",
    "\n",
    "        Arguments:\n",
    "            features {np.ndarray} -- Features to apply means, covariance, and mixing_weights\n",
    "                to.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray -- Posterior probabilities to each Gaussian (shape is\n",
    "                (features.shape[0], self.n_clusters))\n",
    "        \"\"\"\n",
    "\n",
    "        return np.array([self._posterior(features, j) for j in range(self.n_clusters)]).T\n",
    "\n",
    "    def _m_step(self, features, assignments):\n",
    "        \"\"\"\n",
    "        Maximization step in Expectation-Maximization. Given the current features and\n",
    "        assignments, update self.means, self.covariances, and self.mixing_weights. Here,\n",
    "        you implement the update equations for the means, covariances, and mixing weights.\n",
    "            1. Update the means with the mu_j update in Slide 24.\n",
    "            2. Update the mixing_weights with the w_j update in Slide 24\n",
    "            3. Update the covariance matrix with the sigma_j update in Slide 24.\n",
    "\n",
    "        Slide 24 is in these slides: \n",
    "            https://github.com/NUCS349/nucs349.github.io/blob/master/lectures/eecs349_gaussian_mixture_models.pdf\n",
    "\n",
    "        NOTE: When updating the parameters of the Gaussian you always use the output of\n",
    "        the E step taken before this M step (e.g. update the means, mixing_weights, and covariances \n",
    "        simultaneously).\n",
    "\n",
    "        Arguments:\n",
    "            features {np.ndarray} -- Features to update means and covariances, given the\n",
    "                current assignments.\n",
    "            assignments {np.ndarray} -- Soft assignments of each point to one of the cluster,\n",
    "                given by _e_step.\n",
    "\n",
    "        Returns:\n",
    "            means -- Updated means\n",
    "            covariances -- Updated covariances\n",
    "            mixing_weights -- Updated mixing weights\n",
    "        \"\"\"\n",
    "\n",
    "        # INITIALIZE the mixing_weights, means, and covariances\n",
    "        \n",
    "        # The mixing weights are scalars and their count = number of Gaussians\n",
    "        mixing_weights = np.zeros(self.n_clusters)\n",
    "\n",
    "        # The means are vectors of the same dimension as those in the features\n",
    "        # Their count is equal to the number of Gaussian\n",
    "        # The first number in the tuple below is the number of means (=n_clusters)\n",
    "        # The second number is the dimensionality of the means, which is the same as that of the features\n",
    "        means = np.zeros((self.n_clusters, features.shape[-1]))\n",
    "\n",
    "\n",
    "        # Covariances\n",
    "        # This will be a 3d array\n",
    "        # The first dimension is the number of Covariance matrices (we have one per Gaussian)\n",
    "        # The second & third dimensions will be the same, because each covariance matrix is square\n",
    "        # The value of those dimensions is the dimensionality of the data\n",
    "        covariances = np.zeros((self.n_clusters, features.shape[-1], features.shape[-1]))\n",
    "\n",
    "        N  = features.shape[0] # Number of features is frequently used\n",
    "\n",
    "        # Now, we have to update these parameters for EACH GAUSSIAN:\n",
    "\n",
    "        for j in range(self.n_clusters):\n",
    "\n",
    "            # First, we'll calculate GAMMA for this Gaussian, since it appears in all the calculations\n",
    "            GAMMA = sum(assignments[:,j]) # sum of all the responsibilities of a given Gaussian for each of the points in features\n",
    "\n",
    "            # Update the j-th mixing weight\n",
    "            mixing_weights[j] = GAMMA / N\n",
    "\n",
    "            # Update the j-th mean\n",
    "            means[j] = sum([assignments[i,j]*features[i,:] for i in range(N) ]) / GAMMA\n",
    "\n",
    "            # Calculate the j-th covariance matrix\n",
    "            # The below is a bit complicated, but I am looking at page 9 of this: https://campuspro-uploads.s3.us-west-2.amazonaws.com/63aa7cea-5e9c-4b62-96b7-8bbf3bc31b76/3a1d9101-8748-4e85-9830-4e45ffe1ca8d/EM%20derivations.pdf\n",
    "            # The .reshape(-1,1) and reshape(1,-1) is basically to make sure that the result of the dot product is a square matrix.\n",
    "            # It's basically dot product of mean vector minus x vector and the transpose of the same\n",
    "            cov_matrix_j = sum([ assignments[i,j]* np.dot( (self.means[j]-features[i,:]).reshape(-1,1), (self.means[j]-features[i,:]).reshape(1,-1) ) for i in range(N) ]) / GAMMA\n",
    "\n",
    "            # If we are in the spherical case, then we should be able to just take the first element of the covariance matrix\n",
    "            covariances[j] = convert_to_diag(cov_matrix_j)\n",
    "\n",
    "        return means, covariances, mixing_weights\n",
    "\n",
    "    def _init_covariance(self, n_features):\n",
    "        \"\"\"\n",
    "        Initialize the covariance matrix given the covariance_type (spherical or\n",
    "        diagonal). If spherical, each feature is treated the same (has equal covariance).\n",
    "        If diagonal, each feature is treated independently (n_features covariances).\n",
    "\n",
    "        Arguments:\n",
    "            n_features {int} -- Number of features in the data for clustering\n",
    "\n",
    "        Returns:\n",
    "            [np.ndarray] -- Initial covariances (use np.random.rand)\n",
    "        \"\"\"\n",
    "        if self.covariance_type == 'spherical':\n",
    "            return np.random.rand(self.n_clusters)\n",
    "        elif self.covariance_type == 'diagonal':\n",
    "            return np.random.rand(self.n_clusters, n_features)\n",
    "\n",
    "    def _log_likelihood(self, features, k_idx):\n",
    "        \"\"\"\n",
    "        Compute the likelihood of the features given the index of the Gaussian\n",
    "        in the mixture model. This function compute the log multivariate_normal\n",
    "        distribution for features given the means and covariance of the ```k_idx```th\n",
    "        Gaussian. To do this, you can use the function:\n",
    "\n",
    "            scipy.stats.multivariate_normal.logpdf\n",
    "\n",
    "        Read the documentation of this function to understand how it is used here:\n",
    "\n",
    "            https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.multivariate_normal.html\n",
    "\n",
    "        Once the raw likelihood is computed, incorporate the mixing_weights for the Gaussian\n",
    "        via:\n",
    "\n",
    "            log(mixing_weight) + logpdf\n",
    "\n",
    "        Where logpdf is the output of multivariate_normal.\n",
    "\n",
    "        Arguments:\n",
    "            features {np.ndarray} -- Features to compute multivariate_normal distribution\n",
    "                on.\n",
    "            k_idx {int} -- Which Gaussian to use (e.g. use self.means[k_idx], \n",
    "                self.covariances[k_idx], self.mixing_weights[k_idx]).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray -- log likelihoods of each feature given a Gaussian.\n",
    "        \"\"\"\n",
    "\n",
    "        log_pdf = multivariate_normal(mean=self.means[k_idx], cov=self.covariances[k_idx]).logpdf(features)\n",
    "\n",
    "        return np.log(self.mixing_weights[k_idx]) + log_pdf\n",
    "\n",
    "\n",
    "    def _overall_log_likelihood(self, features):\n",
    "\n",
    "        denom = [\n",
    "            self._log_likelihood(features, j) for j in range(self.n_clusters)\n",
    "        ]\n",
    "        return np.sum(denom)\n",
    "\n",
    "    def _posterior(self, features, k):\n",
    "        \"\"\"\n",
    "        Computes the posteriors given the log likelihoods for the GMM. Computes\n",
    "        the posteriors for one of the Gaussians. To get all the posteriors, you have\n",
    "        to iterate over this function. This function is implemented for you because the\n",
    "        numerical issues can be tricky. We use the logsumexp trick to make it work (see\n",
    "        below).\n",
    "\n",
    "        Arguments:\n",
    "            features {np.ndarray} -- Numpy array containing data (n_samples, n_features).\n",
    "            k {int} -- Index of which Gaussian to compute posteriors for.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray -- Posterior probabilities for the selected Gaussian k, of size\n",
    "                (n_samples,).\n",
    "        \"\"\"\n",
    "\n",
    "        num = self._log_likelihood(features, k)\n",
    "\n",
    "        denom = np.array([\n",
    "            self._log_likelihood(features, j)\n",
    "            for j in range(self.n_clusters)\n",
    "        ])\n",
    "\n",
    "        # Below is a useful function for safely computing large exponentials. It's a common\n",
    "        # machine learning trick called the logsumexp trick:\n",
    "        #   https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/\n",
    "\n",
    "        max_value = denom.max(axis=0, keepdims=True)\n",
    "        denom_sum = max_value + np.log(np.sum(np.exp(denom - max_value), axis=0))\n",
    "        posteriors = np.exp(num - denom_sum)\n",
    "        return np.squeeze(posteriors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets = generate_cluster_data(n_samples=100, n_features=2, cluster_stds=.1, n_centers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALLING _OVERALL_LOG_LIKELIHOOD\n",
      "CALLING _LOG_LIKELIHOOD\n",
      "K_IDX= 0\n",
      "SELF.COVARIANCES = [0.45303353 0.34875415]\n",
      "SELF.COVARIANCES[k_idx]= 0.45303352899867877\n",
      "CALLING _LOG_LIKELIHOOD\n",
      "K_IDX= 1\n",
      "SELF.COVARIANCES = [0.45303353 0.34875415]\n",
      "SELF.COVARIANCES[k_idx]= 0.3487541516085212\n",
      "FINISHED CALCULATING _OVERALL_LOG_LIKELIHOOD UNDER INIITAL RANDOM\n",
      "NUMBER OF ITERATION: 0\n",
      "STARTING EXPECTATION STEP\n",
      "CALLING _POSTERIOR\n",
      "CALCULATING NUMERATOR\n",
      "CALLING _LOG_LIKELIHOOD\n",
      "K_IDX= 0\n",
      "SELF.COVARIANCES = [0.45303353 0.34875415]\n",
      "SELF.COVARIANCES[k_idx]= 0.45303352899867877\n",
      "CALCULATING DENOMINATOR\n",
      "CALLING _LOG_LIKELIHOOD\n",
      "K_IDX= 0\n",
      "SELF.COVARIANCES = [0.45303353 0.34875415]\n",
      "SELF.COVARIANCES[k_idx]= 0.45303352899867877\n",
      "CALLING _LOG_LIKELIHOOD\n",
      "K_IDX= 1\n",
      "SELF.COVARIANCES = [0.45303353 0.34875415]\n",
      "SELF.COVARIANCES[k_idx]= 0.3487541516085212\n",
      "CALLING _POSTERIOR\n",
      "CALCULATING NUMERATOR\n",
      "CALLING _LOG_LIKELIHOOD\n",
      "K_IDX= 1\n",
      "SELF.COVARIANCES = [0.45303353 0.34875415]\n",
      "SELF.COVARIANCES[k_idx]= 0.3487541516085212\n",
      "CALCULATING DENOMINATOR\n",
      "CALLING _LOG_LIKELIHOOD\n",
      "K_IDX= 0\n",
      "SELF.COVARIANCES = [0.45303353 0.34875415]\n",
      "SELF.COVARIANCES[k_idx]= 0.45303352899867877\n",
      "CALLING _LOG_LIKELIHOOD\n",
      "K_IDX= 1\n",
      "SELF.COVARIANCES = [0.45303353 0.34875415]\n",
      "SELF.COVARIANCES[k_idx]= 0.3487541516085212\n",
      "STARTING MAXIMIZATION STEP\n",
      "CALCULATED COV_MATRIX_J: [[0.00939724 0.00191416]\n",
      " [0.00191416 0.00870454]]\n",
      "CALCULATED COV_MATRIX_J: [[ 0.00997431 -0.00187076]\n",
      " [-0.00187076  0.00823476]]\n",
      "CALLING _OVERALL_LOG_LIKELIHOOD\n",
      "CALLING _LOG_LIKELIHOOD\n",
      "K_IDX= 0\n",
      "SELF.COVARIANCES = [[[0.00939724 0.        ]\n",
      "  [0.         0.00870454]]\n",
      "\n",
      " [[0.00997431 0.        ]\n",
      "  [0.         0.00823476]]]\n",
      "SELF.COVARIANCES[k_idx]= [[0.00939724 0.        ]\n",
      " [0.         0.00870454]]\n",
      "CALLING _LOG_LIKELIHOOD\n",
      "K_IDX= 1\n",
      "SELF.COVARIANCES = [[[0.00939724 0.        ]\n",
      "  [0.         0.00870454]]\n",
      "\n",
      " [[0.00997431 0.        ]\n",
      "  [0.         0.00823476]]]\n",
      "SELF.COVARIANCES[k_idx]= [[0.00997431 0.        ]\n",
      " [0.         0.00823476]]\n"
     ]
    }
   ],
   "source": [
    "# try spherical\n",
    "covariance_type = 'spherical'\n",
    "\n",
    "model = GMM(c, covariance_type=covariance_type)\n",
    "model.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00939724, 0.        ],\n",
       "       [0.        , 0.00870454]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov0 = model.covariances[0]; cov0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09693935 0.        ]\n",
      " [0.         0.09329811]]\n",
      "[[-0.00306065 -0.1       ]\n",
      " [-0.1        -0.00670189]]\n",
      "[[0.09987148 0.        ]\n",
      " [0.         0.0907456 ]]\n",
      "[[-0.00012852 -0.1       ]\n",
      " [-0.1        -0.0092544 ]]\n"
     ]
    }
   ],
   "source": [
    "for cov in model.covariances:\n",
    "    print(np.sqrt(cov))\n",
    "    print(np.sqrt(cov)-.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009050887405803332"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.diagonal(cov0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[0.00939724 0.        ]\n",
      " [0.         0.00870454]]\n",
      "1\n",
      "[[0.00997431 0.        ]\n",
      " [0.         0.00823476]]\n"
     ]
    }
   ],
   "source": [
    "for index, cov in enumerate(model.covariances):\n",
    "    print(index)\n",
    "    print(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, struct\n",
    "from array import array as pyarray\n",
    "from numpy import append, array, int8, uint8, zeros\n",
    "\n",
    "def load_mnist(dataset=\"training\", digits=None, path=None, asbytes=False, selection=None, return_labels=True, return_indices=False):\n",
    "    \"\"\"\n",
    "    Loads MNIST files into a 3D numpy array.\n",
    "\n",
    "    You have to download the data separately from [MNIST]_. Use the ``path`` parameter\n",
    "    to specify the directory that contains all four downloaded MNIST files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : str\n",
    "        Either \"training\" or \"testing\", depending on which dataset you want to\n",
    "        load.\n",
    "    digits : list\n",
    "        Integer list of digits to load. The entire database is loaded if set to\n",
    "        ``None``. Default is ``None``.\n",
    "    path : str\n",
    "        Path to your MNIST datafiles. The default is ``None``, which will try\n",
    "        to take the path from your environment variable ``MNIST``. The data can\n",
    "        be downloaded from http://yann.lecun.com/exdb/mnist/.\n",
    "    asbytes : bool\n",
    "        If True, returns data as ``numpy.uint8`` in [0, 255] as opposed to\n",
    "        ``numpy.float64`` in [0.0, 1.0].\n",
    "    selection : slice\n",
    "        Using a `slice` object, specify what subset of the dataset to load. An\n",
    "        example is ``slice(0, 20, 2)``, which would load every other digit\n",
    "        until--but not including--the twentieth.\n",
    "    return_labels : bool\n",
    "        Specify whether or not labels should be returned. This is also a speed\n",
    "        performance if digits are not specified, since then the labels file\n",
    "        does not need to be read at all.\n",
    "    return_indicies : bool\n",
    "        Specify whether or not to return the MNIST indices that were fetched.\n",
    "        This is valuable only if digits is specified, because in that case it\n",
    "        can be valuable to know how far\n",
    "        in the database it reached.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    images : ndarray\n",
    "        Image data of shape ``(N, rows, cols)``, where ``N`` is the number of images. If neither labels nor inices are returned, then this is returned directly, and not inside a 1-sized tuple.\n",
    "    labels : ndarray\n",
    "        Array of size ``N`` describing the labels. Returned only if ``return_labels`` is `True`, which is default.\n",
    "    indices : ndarray\n",
    "        The indices in the database that were returned.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Assuming that you have downloaded the MNIST database and set the\n",
    "    environment variable ``$MNIST`` point to the folder, this will load all\n",
    "    images and labels from the training set:\n",
    "\n",
    "    >>> images, labels = ag.io.load_mnist('training') # doctest: +SKIP\n",
    "\n",
    "    Load 100 sevens from the testing set:\n",
    "\n",
    "    >>> sevens = ag.io.load_mnist('testing', digits=[7], selection=slice(0, 100), return_labels=False) # doctest: +SKIP\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # The files are assumed to have these names and should be found in 'path'\n",
    "    files = {\n",
    "        'training': ('train-images-idx3-ubyte', 'train-labels-idx1-ubyte'),\n",
    "        'testing': ('t10k-images-idx3-ubyte', 't10k-labels-idx1-ubyte'),\n",
    "    }\n",
    "\n",
    "    if path is None:\n",
    "        try:\n",
    "            path = os.environ['MNIST']\n",
    "        except KeyError:\n",
    "            raise ValueError(\"Unspecified path requires environment variable $MNIST to be set\")\n",
    "\n",
    "    try:\n",
    "        images_fname = os.path.join(path, files[dataset][0])\n",
    "        labels_fname = os.path.join(path, files[dataset][1])\n",
    "    except KeyError:\n",
    "        raise ValueError(\"Data set must be 'testing' or 'training'\")\n",
    "\n",
    "    # We can skip the labels file only if digits aren't specified and labels aren't asked for\n",
    "    if return_labels or digits is not None:\n",
    "        flbl = open(labels_fname, 'rb')\n",
    "        magic_nr, size = struct.unpack(\">II\", flbl.read(8))\n",
    "        labels_raw = pyarray(\"b\", flbl.read())\n",
    "        flbl.close()\n",
    "\n",
    "    fimg = open(images_fname, 'rb')\n",
    "    magic_nr, size, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "    images_raw = pyarray(\"B\", fimg.read())\n",
    "    fimg.close()\n",
    "\n",
    "    if digits:\n",
    "        indices = [k for k in range(size) if labels_raw[k] in digits]\n",
    "    else:\n",
    "        indices = range(size)\n",
    "\n",
    "    if selection:\n",
    "        indices = indices[selection]\n",
    "    N = len(indices)\n",
    "\n",
    "    images = zeros((N, rows, cols), dtype=uint8)\n",
    "\n",
    "    if return_labels:\n",
    "        labels = zeros((N), dtype=int8)\n",
    "    for i, index in enumerate(indices):\n",
    "        images[i] = array(images_raw[ indices[i]*rows*cols : (indices[i]+1)*rows*cols ]).reshape((rows, cols))\n",
    "        if return_labels:\n",
    "            labels[i] = labels_raw[indices[i]]\n",
    "\n",
    "    if not asbytes:\n",
    "        images = images.astype(float)/255.0\n",
    "\n",
    "    ret = (images,)\n",
    "    if return_labels:\n",
    "        ret += (labels,)\n",
    "    if return_indices:\n",
    "        ret += (indices,)\n",
    "    if len(ret) == 1:\n",
    "        return ret[0] # Don't return a tuple of one\n",
    "    else:\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_mnist(path='mnist', dataset='testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8),\n",
       " array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1135\n",
       "2    1032\n",
       "7    1028\n",
       "3    1010\n",
       "9    1009\n",
       "4     982\n",
       "0     980\n",
       "8     974\n",
       "6     958\n",
       "5     892\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all digits; number for each equal to the minority class (digit \"5\" with 892 counts)\n",
    "ones, ones_labels = load_mnist(path='mnist', dataset='testing', digits=[1], selection=slice(0, min_count), return_labels=True)\n",
    "twos, twos_labels = load_mnist(path='mnist', dataset='testing', digits=[2], selection=slice(0, min_count), return_labels=True)\n",
    "threes, threes_labels = load_mnist(path='mnist', dataset='testing', digits=[3], selection=slice(0, min_count), return_labels=True)\n",
    "fours, fours_labels = load_mnist(path='mnist', dataset='testing', digits=[4], selection=slice(0, min_count), return_labels=True)\n",
    "fives, fives_labels = load_mnist(path='mnist', dataset='testing', digits=[5], selection=slice(0, min_count), return_labels=True)\n",
    "sixs, sixs_labels = load_mnist(path='mnist', dataset='testing', digits=[6], selection=slice(0, min_count), return_labels=True)\n",
    "sevens, sevens_labels = load_mnist(path='mnist', dataset='testing', digits=[7], selection=slice(0, min_count), return_labels=True)\n",
    "eights, eights_labels = load_mnist(path='mnist', dataset='testing', digits=[8], selection=slice(0, min_count), return_labels=True)\n",
    "nines, nines_labels = load_mnist(path='mnist', dataset='testing', digits=[9], selection=slice(0, min_count), return_labels=True)\n",
    "zeroes, zeroes_labels = load_mnist(path='mnist', dataset='testing', digits=[0], selection=slice(0, min_count), return_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping i.e. flattening to 1-D array each digit\n",
    "ones = ones.reshape(min_count, -1)\n",
    "twos = twos.reshape(min_count, -1)\n",
    "threes = threes.reshape(min_count, -1)\n",
    "fours = fours.reshape(min_count, -1)\n",
    "fives = fives.reshape(min_count, -1)\n",
    "sixs = sixs.reshape(min_count, -1)\n",
    "sevens = sevens.reshape(min_count, -1)\n",
    "eights = eights.reshape(min_count, -1)\n",
    "nines = nines.reshape(min_count, -1)\n",
    "zeroes = zeroes.reshape(min_count, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all the digits in one features array\n",
    "digits = np.concatenate((ones, twos, threes, fours, fives, sixs, sevens, eights, nines, zeroes))\n",
    "digits_labels = np.concatenate((ones_labels, twos_labels, threes_labels, fours_labels, fives_labels, sixs_labels, sevens_labels, eights_labels, nines_labels, zeroes_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOO SLOW :/\n",
    "# kmeans = KMeans(10)\n",
    "# kmeans.fit(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import adjusted_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=10)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans.fit(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9, 5, ..., 6, 4, 6], dtype=int32)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_pred = kmeans.predict(digits); kmeans_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture(n_components=10)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm = GaussianMixture(n_components=10)\n",
    "gmm.fit(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 2, 2, 7])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_pred = gmm.predict(digits); gmm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4920112724999069"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_mutual_info_score(digits_labels, kmeans_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5445894986455011"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_mutual_info_score(digits_labels, gmm_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Comparing approaches with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 634), (3, 39), (7, 11), (6, 9), (8, 8), (5, 6), (0, 3), (1, 2), (4, 2), (9, 2)]\n",
      "MOST COMMON LABEL IN THIS CLUSTER:  634\n",
      "TOTAL NUMBER OF OBSERVATIONS IN THIS CLUSTER:  716\n",
      "PROPORTION ACCURACY:  0.8854748603351955\n",
      "[(1, 783), (2, 84), (5, 62), (6, 36), (7, 30), (8, 20), (3, 17), (4, 12), (9, 5), (0, 3)]\n",
      "MOST COMMON LABEL IN THIS CLUSTER:  783\n",
      "TOTAL NUMBER OF OBSERVATIONS IN THIS CLUSTER:  1052\n",
      "PROPORTION ACCURACY:  0.7442965779467681\n",
      "[(0, 359), (6, 41), (5, 15), (8, 8), (9, 6), (4, 4), (2, 2), (3, 1), (7, 1)]\n",
      "MOST COMMON LABEL IN THIS CLUSTER:  359\n",
      "TOTAL NUMBER OF OBSERVATIONS IN THIS CLUSTER:  437\n",
      "PROPORTION ACCURACY:  0.8215102974828375\n",
      "[(7, 645), (9, 32), (2, 18), (8, 14), (3, 8), (5, 4), (0, 1)]\n",
      "MOST COMMON LABEL IN THIS CLUSTER:  645\n",
      "TOTAL NUMBER OF OBSERVATIONS IN THIS CLUSTER:  722\n",
      "PROPORTION ACCURACY:  0.8933518005540166\n",
      "[(3, 478), (5, 191), (8, 108), (2, 48), (0, 23), (6, 5), (1, 4), (9, 3), (7, 1)]\n",
      "MOST COMMON LABEL IN THIS CLUSTER:  478\n",
      "TOTAL NUMBER OF OBSERVATIONS IN THIS CLUSTER:  861\n",
      "PROPORTION ACCURACY:  0.5551684088269454\n",
      "[(8, 672), (5, 505), (3, 326), (1, 92), (2, 52), (4, 43), (9, 36), (0, 35), (6, 33), (7, 17)]\n",
      "MOST COMMON LABEL IN THIS CLUSTER:  672\n",
      "TOTAL NUMBER OF OBSERVATIONS IN THIS CLUSTER:  1811\n",
      "PROPORTION ACCURACY:  0.37106570955273327\n",
      "[(9, 520), (4, 513), (7, 65), (5, 46), (8, 27), (1, 9), (2, 6), (3, 4), (0, 1)]\n",
      "MOST COMMON LABEL IN THIS CLUSTER:  520\n",
      "TOTAL NUMBER OF OBSERVATIONS IN THIS CLUSTER:  1191\n",
      "PROPORTION ACCURACY:  0.436607892527288\n",
      "[(0, 453), (6, 22), (5, 21), (2, 12), (8, 11), (9, 9), (3, 5), (4, 1), (7, 1)]\n",
      "MOST COMMON LABEL IN THIS CLUSTER:  453\n",
      "TOTAL NUMBER OF OBSERVATIONS IN THIS CLUSTER:  535\n",
      "PROPORTION ACCURACY:  0.8467289719626169\n",
      "[(4, 300), (9, 276), (7, 121), (5, 32), (2, 17), (8, 17), (3, 11), (6, 8), (0, 3)]\n",
      "MOST COMMON LABEL IN THIS CLUSTER:  300\n",
      "TOTAL NUMBER OF OBSERVATIONS IN THIS CLUSTER:  785\n",
      "PROPORTION ACCURACY:  0.3821656050955414\n",
      "[(6, 738), (2, 19), (4, 17), (0, 11), (5, 10), (8, 7), (3, 3), (9, 3), (1, 2)]\n",
      "MOST COMMON LABEL IN THIS CLUSTER:  738\n",
      "TOTAL NUMBER OF OBSERVATIONS IN THIS CLUSTER:  810\n",
      "PROPORTION ACCURACY:  0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "weighted_accuracy = 0\n",
    "total_counts = 0\n",
    "\n",
    "for cluster_value in range(10):\n",
    "    values_counts = np.unique(digits_labels[gmm_pred == cluster_value], return_counts=True)\n",
    "    values = values_counts[0]\n",
    "    counts = values_counts[1]\n",
    "    tuples = sorted(list(zip(values, counts)), key = lambda x: x[1], reverse=True) ; \n",
    "    print(tuples)\n",
    "    print(\"MOST COMMON LABEL IN THIS CLUSTER: \",tuples[0][1])\n",
    "    print(\"TOTAL NUMBER OF OBSERVATIONS IN THIS CLUSTER: \", sum(counts))\n",
    "    print(\"PROPORTION ACCURACY: \", tuples[0][1]/sum(counts))\n",
    "    weighted_accuracy += tuples[0][1]\n",
    "    total_counts += sum(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8920"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6257847533632287"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_accuracy/total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "value = 0\n",
    "values_counts = np.unique(digits_labels[kmeans_pred == value], return_counts=True)\n",
    "values = values_counts[0]\n",
    "counts = values_counts[1]\n",
    "tuples = sorted(list(zip(values, counts)), key = lambda x: x[1], reverse=True) ; tuples\n",
    "print(tuples[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METHOD 1 - Visualizing Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(v, w):\n",
    "    \"\"\"Returns the Euclidean distance between two vectors\"\"\"\n",
    "\n",
    "    return math.sqrt(sum([(v[ii]-w[ii])**2 for ii in range(len(v))]))\n",
    "\n",
    "def euclidean_distances(X, Y):\n",
    "    \"\"\"Compute pairwise Euclidean distance between the rows of two matrices X (shape MxK) \n",
    "    and Y (shape NxK). The output of this function is a matrix of shape MxN containing\n",
    "    the Euclidean distance between two rows.\n",
    "    \n",
    "    Arguments:\n",
    "        X {np.ndarray} -- First matrix, containing M examples with K features each.\n",
    "        Y {np.ndarray} -- Second matrix, containing N examples with K features each.\n",
    "\n",
    "    Returns:\n",
    "        D {np.ndarray}: MxN matrix with Euclidean distances between rows of X and rows of Y.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = np.ndarray((X.shape[0], Y.shape[0]))\n",
    "    \n",
    "    for ii in range(X.shape[0]):\n",
    "        for kk in range(Y.shape[0]):\n",
    "            \n",
    "            result[ii,kk] = euclidean_distance(X[ii], Y[kk])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_labels = [2,1,0,7,3,8,9,0,4,6]\n",
    "\n",
    "for cluster_value in range(10):\n",
    "    cluster_digits = digits[gmm_pred == cluster_value,:];\n",
    "    cluster_mean = np.mean(cluster_digits, axis=0)\n",
    "    cluster_mean = cluster_mean.reshape(28,28)\n",
    "    img = Image.fromarray(np.uint8(cluster_mean*255), 'L')\n",
    "    img.save(f'cluster_value_{majority_labels[cluster_value]}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_value in range(10):\n",
    "    cluster_digits = digits[gmm_pred == cluster_value,:]; cluster_digits.shape\n",
    "    cluster_mean = np.mean(cluster_digits, axis=0);\n",
    "    cluster_mean = cluster_mean.reshape(1,-1)\n",
    "    distances = euclidean_distances(cluster_digits, cluster_mean)\n",
    "    closest_neighbour = cluster_digits[np.argmin(distances),:]\n",
    "    closest_neighbour = closest_neighbour.reshape(28,28)\n",
    "    img = Image.fromarray(np.uint8(closest_neighbour*255), 'L')\n",
    "    img.save(f'closest_neighbour_{majority_labels[cluster_value]}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will take all the 2s\n",
    "all_twos = load_mnist(path='mnist', dataset='testing', digits=[2], selection=slice(0, 1032), return_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1032, 28, 28)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_twos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1032, 784)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_twos = all_twos.reshape(1032, -1); all_twos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture(n_components=50)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate and fit models\n",
    "gmm_1 = GaussianMixture(n_components=1)\n",
    "gmm_4 = GaussianMixture(n_components=4)\n",
    "gmm_10 = GaussianMixture(n_components=10)\n",
    "gmm_20 = GaussianMixture(n_components=20)\n",
    "gmm_50 = GaussianMixture(n_components=50)\n",
    "gmm_1.fit(all_twos)\n",
    "gmm_4.fit(all_twos)\n",
    "gmm_10.fit(all_twos)\n",
    "gmm_20.fit(all_twos)\n",
    "gmm_50.fit(all_twos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_1_samples = gmm_1.sample(5)[0]\n",
    "gmm_4_samples = gmm_4.sample(5)[0]\n",
    "gmm_10_samples = gmm_10.sample(5)[0]\n",
    "gmm_20_samples = gmm_20.sample(5)[0]\n",
    "gmm_50_samples = gmm_50.sample(5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_1_samples = gmm_1_samples.reshape(5,28,28)\n",
    "gmm_4_samples = gmm_4_samples.reshape(5,28,28)\n",
    "gmm_10_samples = gmm_10_samples.reshape(5,28,28)\n",
    "gmm_20_samples = gmm_20_samples.reshape(5,28,28)\n",
    "gmm_50_samples = gmm_50_samples.reshape(5,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, arr in enumerate(gmm_1_samples):\n",
    "    img = Image.fromarray(np.uint8(arr*255), 'L')\n",
    "    img.save(f'gmm_1_{index}.png')\n",
    "    \n",
    "for index, arr in enumerate(gmm_4_samples):\n",
    "    img = Image.fromarray(np.uint8(arr*255), 'L')\n",
    "    img.save(f'gmm_4_{index}.png')\n",
    "    \n",
    "for index, arr in enumerate(gmm_10_samples):\n",
    "    img = Image.fromarray(np.uint8(arr*255), 'L')\n",
    "    img.save(f'gmm_10_{index}.png')\n",
    "    \n",
    "for index, arr in enumerate(gmm_20_samples):\n",
    "    img = Image.fromarray(np.uint8(arr*255), 'L')\n",
    "    img.save(f'gmm_20_{index}.png')\n",
    "    \n",
    "for index, arr in enumerate(gmm_50_samples):\n",
    "    img = Image.fromarray(np.uint8(arr*255), 'L')\n",
    "    img.save(f'gmm_50_{index}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [0 1]]\n",
      "[[2 0]\n",
      " [0 2]]\n",
      "[[3 0]\n",
      " [0 3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1, 0],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[2, 0],\n",
       "        [0, 2]],\n",
       "\n",
       "       [[3, 0],\n",
       "        [0, 3]]])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov1 = np.array([[1,0],[0,1]]); print(cov1)\n",
    "cov2 = np.array([[2,0],[0,2]]); print(cov2)\n",
    "cov3 = np.array([[3,0],[0,3]]); print(cov3)\n",
    "\n",
    "array_of_cov = np.array([cov1, cov2, cov3]); array_of_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_diag(x):\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if i != j:\n",
    "                x[i,j]=0\n",
    "                \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01031506, 0.        ],\n",
       "       [0.        , 0.01070544]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01031506, 0.        ],\n",
       "       [0.        , 0.01070544]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_diag(cov1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 4, 6],\n",
       "       [3, 6, 9]])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(v.reshape(-1,1), v.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9, 9, 9],\n",
       "       [9, 9, 9],\n",
       "       [9, 9, 9]])"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot((v-w).reshape(-1,1), (v-w).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.array([1,2,3])\n",
    "w = np.array([4,5,6])\n",
    "np.dot(v.T, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((5, features.shape[-1], features.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments = np.array([[[1.21961890e-42, 1.00000000e+00 ,1.00000000e+00 ,1.00000000e+00,\n",
    "   3.64109720e-41, 1.00000000e+00, 1.00000000e+00, 3.74421045e-41,\n",
    "   4.87806565e-41, 1.45958336e-40, 1.00000000e+00, 1.00000000e+00,\n",
    "   4.26006989e-41, 1.00000000e+00, 1.00000000e+00, 3.17551087e-42,\n",
    "   1.00000000e+00, 9.57973014e-42, 4.25792685e-42, 1.00000000e+00,\n",
    "   1.00000000e+00, 2.09756900e-41, 3.01459744e-41, 1.00000000e+00,\n",
    "   1.00000000e+00, 3.80978654e-42, 1.40268657e-40, 1.00000000e+00,\n",
    "   1.00000000e+00, 1.00000000e+00, 3.71658618e-41, 1.62517789e-41,\n",
    "   8.64416847e-42, 1.00000000e+00, 1.00000000e+00, 1.70475662e-42,\n",
    "   8.93523471e-42, 1.00000000e+00, 2.85155218e-41, 1.00000000e+00,\n",
    "   1.47889899e-41, 1.07193063e-40 ,8.05744808e-43 ,1.27125030e-40,\n",
    "   7.56737129e-42, 1.00000000e+00 ,5.64624347e-41 ,1.00000000e+00,\n",
    "   1.00000000e+00, 1.00000000e+00 ,9.76127495e-42 ,1.00000000e+00,\n",
    "   1.00000000e+00, 9.76628231e-41, 1.00000000e+00, 1.00000000e+00,\n",
    "   1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 2.43331062e-41,\n",
    "   1.00000000e+00, 1.49012901e-40, 8.78488390e-42, 1.00000000e+00,\n",
    "   2.69950682e-41, 2.63120784e-41, 1.00000000e+00, 1.00000000e+00,\n",
    "   1.00000000e+00, 1.00000000e+00 ,1.00000000e+00, 1.61649630e-42,\n",
    "   2.40472860e-41, 2.19837867e-41 ,7.36717651e-41, 3.71782922e-41,\n",
    "   1.00000000e+00, 1.04890778e-42 ,3.82693869e-41, 1.00000000e+00,\n",
    "   1.00000000e+00, 1.29462646e-42 ,1.73631066e-41, 4.17001364e-42,\n",
    "   1.00000000e+00, 1.63493222e-41 ,1.41022234e-41 ,1.48114241e-42,\n",
    "   1.00000000e+00, 1.00000000e+00 ,2.64987929e-41, 4.62333192e-42,\n",
    "   1.00000000e+00, 3.05530655e-42 ,1.00000000e+00, 3.08212318e-41,\n",
    "   1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 8.91698359e-42,]],\n",
    "\n",
    " [[1.00000000e+00, 2.78271783e-43, 2.02018793e-42, 4.34626054e-43,\n",
    "   1.00000000e+00, 3.85261403e-42, 5.51141032e-43, 1.00000000e+00,\n",
    "   1.00000000e+00, 1.00000000e+00, 1.22548705e-43, 8.21242211e-44,\n",
    "   1.00000000e+00, 8.94371949e-43, 2.62672872e-44, 1.00000000e+00,\n",
    "   3.20165586e-43, 1.00000000e+00, 1.00000000e+00, 6.34215241e-43,\n",
    "   1.86760186e-42, 1.00000000e+00, 1.00000000e+00, 5.07054323e-43,\n",
    "   4.34270002e-42, 1.00000000e+00, 1.00000000e+00, 8.32453352e-43,\n",
    "   1.75272807e-43, 8.16890915e-44, 1.00000000e+00, 1.00000000e+00,\n",
    "   1.00000000e+00, 6.56011169e-42, 5.53140023e-43, 1.00000000e+00,\n",
    "   1.00000000e+00 ,1.15123579e-42, 1.00000000e+00, 2.31744221e-43,\n",
    "   1.00000000e+00 ,1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
    "   1.00000000e+00 ,4.81599640e-43, 1.00000000e+00, 6.00548795e-43,\n",
    "   6.76975603e-43, 2.89241589e-42, 1.00000000e+00, 1.80452371e-42,\n",
    "   4.91054468e-43, 1.00000000e+00, 2.02993415e-41, 2.63147335e-42,\n",
    "   3.34141148e-43, 3.81143296e-44, 9.29447183e-42, 1.00000000e+00,\n",
    "   1.02679591e-43, 1.00000000e+00, 1.00000000e+00, 1.10244580e-42,\n",
    "   1.00000000e+00, 1.00000000e+00, 1.16915424e-42, 2.75953275e-42,\n",
    "   7.65203746e-43, 1.73554181e-43, 1.03698828e-41, 1.00000000e+00,\n",
    "   1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
    "   6.88048219e-42, 1.00000000e+00, 1.00000000e+00, 3.93334887e-42,\n",
    "   1.85089884e-42, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
    "   4.70978256e-43, 1.00000000e+00 ,1.00000000e+00, 1.00000000e+00,\n",
    "   1.49685829e-42, 5.44502878e-42 ,1.00000000e+00, 1.00000000e+00,\n",
    "   5.39553426e-43, 1.00000000e+00 ,2.36988071e-42, 1.00000000e+00,\n",
    "   3.94045528e-43, 1.86503845e-42 ,4.22258402e-44, 1.00000000e+00]]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw3-gmm",
   "language": "python",
   "name": "hw3-gmm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
